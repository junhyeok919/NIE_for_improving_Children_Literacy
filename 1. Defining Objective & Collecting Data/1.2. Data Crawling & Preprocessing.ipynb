{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa6cd8e-461c-4c3d-8deb-f79fe3d335e6",
   "metadata": {},
   "source": [
    "## 🔖목차\n",
    "- [1. Selenium 기반 데이터 크롤링](#1.-Selenium-기반-데이터-크롤링)\n",
    "  - [크롤링 수행](#크롤링-수행)\n",
    "- [2. 데이터셋 취합](#2.-데이터셋-취합)\n",
    "  - [데이터 병합](#데이터-병합)\n",
    "  - [누락 행 확인](#누락-행-확인)\n",
    "- [3. 텍스트 전처리](#3.-텍스트-전처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81cbc6",
   "metadata": {},
   "source": [
    "# 1. Selenium 기반 데이터 크롤링\n",
    "- BigkindsCrawler 클래스 & 함수를 만들어서 웹 크롤링 작업을 자동화함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ad551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class BigkindsCrawler:\n",
    "    def __init__(self, path, year):\n",
    "        self.df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        self.year = year\n",
    "        # 각 월 별 날짜 수 (Hard-coded) & 각 월\n",
    "        self.months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "        self.days = [\"31\", \"28\", \"31\", \"30\", \"31\", \"30\", \"31\", \"31\", \"30\", \"31\", \"30\", \"31\"]\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.crawled_df = pd.DataFrame(columns=[\"일자\", \"언론사\", \"제목\", \"URL\", \"본문\"])\n",
    "\n",
    "    # set the WebDriver options\n",
    "    def set_driver_options(self):\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        # set User-Agent for preventing access blocked\n",
    "        self.options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\" +\n",
    "                                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "        # prevent webdriver from closing immediately\n",
    "        self.options.add_experimental_option(\"detach\", True)\n",
    "        # 크롬 브라우저가 직접적으로 열리지 않도록 설정\n",
    "        self.options.add_argument('--headless')\n",
    "        # 불필요한 이미지 로딩 없앰 (시간 단축)\n",
    "        self.options.add_argument('--disable-logging')\n",
    "        self.options.add_argument('--disable-images')\n",
    "\n",
    "    # csv 파일 필요: publisher, keyword를 csv로 먹임\n",
    "    # category: 통합 분류 (li: 정치=1, 경제=2, 사회=3, 국제=5)\n",
    "    def executor(self, publisher, m, category, keyword):\n",
    "        res = []\n",
    "\n",
    "        start_day = self.year + \"-\" + self.months[m] + \"-\" + \"01\"\n",
    "        end_day = self.year + \"-\" + self.months[m] + \"-\" + self.days[m]\n",
    "\n",
    "        # webdriver 생성\n",
    "        driver = webdriver.Chrome(options=self.options)\n",
    "        driver.get(\"https://www.bigkinds.or.kr/v2/news/index.do\")\n",
    "\n",
    "        # 언론사 클릭\n",
    "        pub = self.transform_publisher(publisher)\n",
    "        driver.find_element(By.XPATH, f\"//*[@id='category_provider_list']/li[{pub}]/span/label\").click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 기간 클릭 (배너)\n",
    "        driver.find_element(By.XPATH, \"//*[@id='collapse-step-1-body']/div[3]/div/div[1]/div[1]/a\").click()\n",
    "        # 기간 클릭 (1개월)\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[1]/span[3]/label\").click()\n",
    "\n",
    "        # 시작 날짜 클릭\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[2]/div/div[1]/img\").click()\n",
    "        start = driver.find_element(By.XPATH, \"//*[@id='search-begin-date']\")\n",
    "        start.send_keys(Keys.CONTROL, 'a')\n",
    "        start.send_keys(start_day)\n",
    "\n",
    "        # 종료 날짜 클릭\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[2]/div/div[3]/img\").click()\n",
    "        end = driver.find_element(By.XPATH, \"//*[@id='search-end-date']\")\n",
    "        end.send_keys(Keys.CONTROL, 'a')\n",
    "        end.send_keys(end_day)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 통합 분류 클릭 (배너)\n",
    "        # 그냥 클릭하면 페이지 로딩 시간 때문에 오류가 날 수 있어서 webdriver 기다림\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='collapse-step-1-body']/div[3]/div/div[2]/div[1]/a\"))\n",
    "        )\n",
    "        element.click()\n",
    "\n",
    "        # 통합 분류 (li: 정치=1, 경제=2, 사회=3, 국제=5)\n",
    "        driver.find_element(By.XPATH, f\"//*[@id='srch-tab3']/ul/li[{category}]/div/span[4]\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 키워드 입력: 오류 나지 않게 한 글자씩 입력함\n",
    "        keyword_input = driver.find_element(By.XPATH, \"//*[@id='total-search-key']\")\n",
    "        for k in keyword:\n",
    "            keyword_input.send_keys(k)\n",
    "            time.sleep(0.2)\n",
    "        keyword_input.send_keys(Keys.RETURN)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 정확도순\n",
    "        driver.find_element(By.XPATH, \"//*[@id='select1']/option[2]\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            # 맨 위의 기사 클릭\n",
    "            driver.find_element(By.XPATH, \"//*[@id='news-results']/div[1]/div/div[2]\").click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # \"일자\", \"언론사\", \"제목\", \"URL\", \"본문\"\n",
    "            try:\n",
    "                date = driver.find_element(By.XPATH,\n",
    "                                           \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/div[1]/ul/li[1]\").text\n",
    "            except NoSuchElementException:\n",
    "                date = \"N/A\"\n",
    "            \n",
    "            title = driver.find_element(By.XPATH, \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/h1\").text\n",
    "\n",
    "            # URL 오류 처리\n",
    "            href_button = driver.find_element(By.XPATH,\n",
    "                                              \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/div[2]/div[1]/button[1]\")\n",
    "\n",
    "            if href_button.text == \"기사원문\":\n",
    "                href = href_button.get_attribute(\"onclick\")\n",
    "\n",
    "                # ?가 포함되었을 경우, 쿼리 문자열이므로 뒤의 문자열은 삭제\n",
    "                try:\n",
    "                    url = re.search(r'https?://[^?]+', href).group()\n",
    "                except AttributeError:\n",
    "                    # URL이 매치되지 않는 경우, 예외 처리를 통해 http 이후의 문자열만 저장\n",
    "                    url = re.search(r'https?://+', href).group()\n",
    "            else:\n",
    "                url = \"N/A\"\n",
    "\n",
    "            paper = driver.find_element(By.XPATH, \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[2]\")\n",
    "            main_text = paper.text\n",
    "        except NoSuchElementException:\n",
    "            date = \"N/A\"\n",
    "            publisher = \"N/A\"\n",
    "            title = \"N/A\"\n",
    "            url = \"N/A\"\n",
    "            main_text = \"N/A\"\n",
    "\n",
    "        res.append(date)\n",
    "        res.append(publisher)\n",
    "        res.append(title)\n",
    "        res.append(url)\n",
    "        res.append(main_text)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    # 월 단위별로 크롤링\n",
    "    def crawling(self, MONTH):\n",
    "        s_index = (MONTH - 1) * 16\n",
    "        size = 16\n",
    "        publishers = self.df.loc[s_index:s_index + size, \"언론사\"]\n",
    "\n",
    "        categories = self.df.loc[s_index:s_index + size, \"카테고리\"]\n",
    "\n",
    "        total_time = 0\n",
    "\n",
    "        for i in range(s_index, s_index + size):\n",
    "            rank_str = self.df.loc[i, \"top-10 키워드\"]\n",
    "            rank_str = rank_str.replace(\"'\", '\"')\n",
    "\n",
    "            # JSON 문자열을 파이썬 리스트로 변환\n",
    "            data_list = json.loads(rank_str)\n",
    "            data_list = data_list[:5]\n",
    "\n",
    "            keywords = [item['name'] for item in data_list]\n",
    "\n",
    "            publisher = publishers[i]\n",
    "            category = categories[i] // 1000000\n",
    "\n",
    "            # 변수 체크용\n",
    "            print(f\"CSV 행 = {i}\")\n",
    "            print(f\"언론사 = {publisher}\")\n",
    "            print(f\"카테고리 = {category}\")\n",
    "\n",
    "            for j, keyword in enumerate(keywords):\n",
    "                s = time.time()\n",
    "                print(\n",
    "                    f\"{'>>>>>  * Process: ' + str(MONTH) + 'th month ' + str(i * 5 + j + 1) + 'th/' + '960th *  <<<<<':^50}\")\n",
    "                self.crawled_df.loc[i * 5 + j, :] = self.executor(publisher, MONTH - 1, category, keyword)\n",
    "                e = time.time()\n",
    "                total_time += round(e - s, 2)\n",
    "                print(f\"누적 소요 시간: {total_time:.2f}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "    def transform_publisher(self, p):\n",
    "        pub = 0\n",
    "        if p == \"경향신문\":\n",
    "            pub = 1\n",
    "        elif p == \"동아일보\":\n",
    "            pub = 4\n",
    "        elif p == \"조선일보\":\n",
    "            pub = 8\n",
    "        elif p == \"중앙일보\":\n",
    "            pub = 9\n",
    "        elif p == \"한겨레\":\n",
    "            pub = 10\n",
    "\n",
    "        return pub\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.crawled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954b195",
   "metadata": {},
   "source": [
    "## 크롤링 수행\n",
    "- 실제 크롤링 수행하는 코드\n",
    "- 연도를 입력하면 총 12달에 대한 기사 본문 텍스트를 각각 csv 파일로 저장해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dddc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YEAR = \"2014\" #👈여기에 크롤링할 연도를 입력해주세요.\n",
    "\n",
    "crawler_2013 = BigkindsCrawler(f\"topkeywords_{YEAR}.csv\", YEAR)\n",
    "crawler_2013.set_driver_options()  # 옵션 세팅\n",
    "\n",
    "# 1 ~ 12월까지 크롤링하고, 각 월 별로 데이터 프레임을 만듭니다.\n",
    "for Month in range(1, 13):\n",
    "    crawler_2013.crawling(Month)\n",
    "    dataframe = crawler_2013.get_df()\n",
    "    # 월별로 csv 추출도 진행합니다.\n",
    "    dataframe.to_csv(f\"{YEAR}_{Month}.csv\", encoding=\"utf-8-sig\")\n",
    "    print(f\"{YEAR}_{Month}.csv complete! \\n\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_2013.crawled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea7495-ef45-49b3-9f28-9d6d9edec2d9",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 취합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8f3ae-9b9a-4e37-b1cd-952afd59e1ed",
   "metadata": {},
   "source": [
    "## 데이터 병합\n",
    "- 월별로 수집한 데이터셋을 전부 하나로 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac12bb-ac17-4ecf-ba29-5428aa24f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 월별 데이터프레임을 저장할 공간\n",
    "dataframes = []\n",
    "\n",
    "# 2013년부터 2022년까지 불러오기\n",
    "for year in range(2013, 2023):\n",
    "    for month in range(1, 13):\n",
    "        filename = f\"{year}_{month}.csv\"\n",
    "        df = pd.read_csv(filename, encoding=\"utf-8-sig\")\n",
    "        dataframes.append(df)\n",
    "        \n",
    "\n",
    "# 행을 기준으로 결합\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2a812-a517-4ece-8180-ba3f91377635",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframes) # 12개월 * 9년치 = 84 맞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a54e9-16da-431a-b1cc-c83069d06a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80364a5-5935-4587-a9d8-f1fb39e6cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[['언론사','제목','본문','일자','URL']] # 필요한 컬럼만 남김\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0d69d-d80f-4ad9-9395-081bf702c752",
   "metadata": {},
   "source": [
    "➡️1년에 960행 * 10년치 = 9,600 행 맞음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e06be8-47b5-4a83-92ce-434aa335dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '카테고리' 컬럼 다시 추가 (by 재언)\n",
    "combined_df = pd.read_csv(\"9600 카테고리.csv\", encoding=\"utf-8-sig\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69034426-e32c-45aa-8aeb-cda75d39efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_politics = combined_df['카테고리'].value_counts().get('국제', 0)\n",
    "count_politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9562c-69be-4cf2-a7f3-aee791547152",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[['언론사','제목','카테고리','본문','일자','URL']]\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7d050-241f-481b-bfc2-53f5a01e07ba",
   "metadata": {},
   "source": [
    "## 누락 행 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805a797-413d-4fcf-af3c-d9b1dec17064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '본문' column에 누락된 경우 있는지 확인\n",
    "missing_rows = combined_df[combined_df.iloc[:, 3].isna()]\n",
    "\n",
    "missing_rows = pd.DataFrame(missing_rows)\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3200ee-fba2-4fa2-8f2e-a85ea61b6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087b989-4801-415a-a5a2-04a5961a5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset=['본문'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d6fc2-7a57-4f6c-9274-bd1dd06c13d9",
   "metadata": {},
   "source": [
    "# 3. 텍스트 전처리\n",
    "- 한국어 텍스트 난이도를 평가하는 데에 방해가 되는 요소(특수기호, 한자 및 일본어, 이메일 및 사이트 주소, 각종 괄호)를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487856b6-5f12-4565-9644-d06022ca6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('C:/Users/simon/PythonWorkspace/Psat_Datamining')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f9779-d198-4182-a341-5d9bc6583d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    # text가 문자열이 아니면 그대로 반환\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # (/br)로 나오는 경우 제거\n",
    "    text = text.replace('(/br)', ' ')\n",
    "    # 특수 문자 지정 후 제거\n",
    "    text = re.sub(r'[☞▶◆#⊙※△▽▼□■◇◎☎○]+', ' ', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'〃', ' ', text)  \n",
    "    # 한자 및 일본어 제거\n",
    "    text = re.sub(r'[\\p{Script=Hiragana}\\p{Script=Katakana}\\p{Script=Han}]+', ' ', text, flags=re.UNICODE)\n",
    "    # 이메일 주소 제거\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    # 사이트 주소 제거(www. 으로 시작하고 .kr로 끝나는 경우)\n",
    "    text = re.sub(r'www\\..+\\.kr', ' ', text)\n",
    "    \n",
    "    # 대괄호로 둘러싸인 내용을 삭제 (10글자 미만인 경우는 삭제, 10글자 이상인 경우는 유지)\n",
    "    text = re.sub(r'\\[([^\\]]{1,9})\\]', ' ', text)\n",
    "    text = re.sub(r'\\[([^\\]]{10,})\\]', r'\\1', text)\n",
    "    # \"<...>\"로 둘러싸인 내용을 삭제\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    # 소괄호 안에 아무런 내용도 없으면 삭제\n",
    "    text = re.sub(r'\\(\\s*\\)', ' ', text)\n",
    "    \n",
    "    # 마지막으로 space가 여러 번 있는 경우를 전부 단일 space로 정리!\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_news(df, column_name = '본문'):\n",
    "    df[column_name] = df[column_name].apply(cleaning_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699685aa-5821-43aa-9498-e3b4935fd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = preprocess_news(combined_df)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1130f-0dbc-406f-b0d6-1c4d5965b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 누락된 행 없는지 재확인\n",
    "missing_rows = combined_df[combined_df.iloc[:, 3].isna()]\n",
    "missing_rows = pd.DataFrame(missing_rows)\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fed3e-0c8f-4cfd-8e85-bb9431b7dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일로 최종 추출!!\n",
    "combined_df.to_csv(\"~~데이터셋 취합본~~.csv\", encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
