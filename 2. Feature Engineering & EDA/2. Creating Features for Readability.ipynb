{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ëª©ì°¨\n",
    "**1. ë°”ë¥¸ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ì¸¡ì •í•œ ë³€ìˆ˜**\n",
    "- ë¬¸ì¥ ê°œìˆ˜, í˜•íƒœì†Œ ê°œìˆ˜\n",
    "- ì¼ìƒ ì¶œí˜„ ë¹ˆë„(ì˜¨ë¼ì¸), ì‹ ë¬¸ ì¶œí˜„ ë¹ˆë„(ë¬¼ê²°21), ê¸°ì´ˆ ì–´íœ˜ ë‚œì´ë„\n",
    "- ìˆ˜ì‹ì–¸ ê°œìˆ˜, ì „ì„±ì–´ë¯¸ ê°œìˆ˜, ì—°ê²°ì–´ë¯¸ ê°œìˆ˜, ì§€ì‹œí‘œí˜„ ê°œìˆ˜\n",
    "\n",
    "**2. ìŒìš´ë¡ ì  ë³µì¡ë„ ë³€ìˆ˜**\n",
    "\n",
    "**3. ìŒì ˆ ê°œìˆ˜, ë‹¨ì–´ ê°œìˆ˜**\n",
    "\n",
    "**4. íŒŒìƒë³€ìˆ˜ ìƒì„±**\n",
    "- í‰ê·  ë¬¸ì¥ ê¸¸ì´, í‰ê·  ë‹¨ì–´ ê¸¸ì´\n",
    "- ê¾¸ë°ˆí‘œí˜„ ë“±ì¥ë¹„ìœ¨, ì•ˆì€ë¬¸ì¥ ë“±ì¥ë¹„ìœ¨, ì´ì–´ì§„ë¬¸ì¥ ë“±ì¥ë¹„ìœ¨, ì§€ì‹œí‘œí˜„ ë“±ì¥ë¹„ìœ¨\n",
    "\n",
    "**5. NF-iDF (News Frequenct - inverse Daily Frequency)**\n",
    "\n",
    "**6. ì¼ìƒ ì¶œí˜„ ë¹ˆë„(êµ­ë¦½êµ­ì–´ì›)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import bareunpy\n",
    "from bareunpy import Tagger\n",
    "\n",
    "from jamo import h2j, j2hcj\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ1. ë°”ë¥¸ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ì¸¡ì •í•œ ë³€ìˆ˜\n",
    "- ë¬¸ì¥ ê°œìˆ˜, í˜•íƒœì†Œ ê°œìˆ˜\n",
    "- ì¼ìƒ ì¶œí˜„ ë¹ˆë„, ì‹ ë¬¸ ì¶œí˜„ ë¹ˆë„(ë¬¼ê²°21), ê¸°ì´ˆ ì–´íœ˜ ë‚œì´ë„\n",
    "- ìˆ˜ì‹ì–¸ ê°œìˆ˜, ì „ì„±ì–´ë¯¸ ê°œìˆ˜, ì—°ê²°ì–´ë¯¸ ê°œìˆ˜, ì§€ì‹œí‘œí˜„ ê°œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°”ë¥¸ API ì¤€ë¹„ \n",
    "API_KEY=\"ë³¸ì¸ì˜ API Key ì…ë ¥\" \n",
    "my_tagger = Tagger(API_KEY, 'localhost') # Tagger ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•„ìš”í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â­ì‹ ë¬¸ ê¸°ì‚¬ ì „ì²´ ë°ì´í„°ì…‹ (10ë…„ì¹˜ ì·¨í•©ë³¸)\n",
    "data = pd.read_csv('~~ì‹ ë¬¸ ê¸°ì‚¬ ì „ì²´ ë°ì´í„°ì…‹~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# ì¼ìƒë¹ˆë„ ë°ì´í„°\n",
    "daily_corp = pd.read_csv('~~ì¼ìƒë¹ˆë„ ë°ì´í„°~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# ë¬¼ê²°21 ë¹ˆë„ ë°ì´í„°\n",
    "wave_corp = pd.read_csv('~~ë¬¼ê²°21 ë¹ˆë„ ë°ì´í„°~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# êµ­ë¦½êµ­ì–´ì› ê¸°ì´ˆì–´íœ˜ ë°ì´í„°\n",
    "basic_corp = pd.read_csv('~~êµ­ë¦½êµ­ì–´ì› ê¸°ì´ˆì–´íœ˜ ë°ì´í„°~~.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ë¬¸ê¸°ì‚¬ ë°ì´í„°ì…‹ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì‹ ë¬¸ ê¸°ì‚¬ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "data = data8565[['ì–¸ë¡ ì‚¬', 'ë³¸ë¬¸']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³€ìˆ˜ ìƒì„±ì— ì‚¬ìš©í•  ë°ì´í„°(ì¼ìƒë¹ˆë„, ë¬¼ê²°21, êµ­ë¦½êµ­ì–´ì› ê¸°ì´ˆì–´íœ˜)ë„ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ìƒë¹ˆë„ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "daily_corp = daily_corp[daily_corp['frequency'] >= np.mean(daily_corp['frequency'])]\n",
    "daily_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¼ê²°21 ë°ì´í„°ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "wave_corp = wave_corp[['íƒœê·¸', 'ë‹¨ì–´', 'ì´ ë¹ˆë„ìˆ˜']]\n",
    "wave_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì´ˆì–´íœ˜ ë‚œì´ë„ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "basic_corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ë³¸ë¬¸] ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ì— loop ëŒë©´ì„œ í•„ìš”í•œ ë³€ìˆ˜ë“¤ ìƒì„±í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ í’ˆì‚¬ íƒœê·¸ ì§€ì • -> ìˆ˜ì‹ì–¸, ì—°ê²°ì–´ë¯¸, ì „ì„±ì–´ë¯¸, ì§€ì‹œí‘œí˜„\n",
    "\n",
    "susik_set = ['MMA', 'MAG'] # ì ‘ì†ë¶€ì‚¬ MAJ ì œì™¸\n",
    "junsung_set = ['ETN', 'ETM'] # ì „ì„±ì–´ë¯¸\n",
    "yeongyeol_set = 'EC' # ì—°ê²°ë¯¸ë¯¸\n",
    "jisi_set = ['NP', 'MMD', 'MAJ'] # ì§€ì‹œí‘œí˜„\n",
    "important_pos_list = [\"NNG\", 'NNP', \"NNB\", \"NP\", \"NR\", \"NF\", \"NA\", \"NV\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"MMA\", \"MMD\", \"MMN\", \"MAG\", \"MAJ\"] # ê¸°ì´ˆì–´íœ˜ í’ˆì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = time.time()\n",
    "\n",
    "# ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ \n",
    "average_usage_list = []\n",
    "count_list = []\n",
    "susik_list = []\n",
    "junsung_list = []\n",
    "yeongyeol_list = []\n",
    "jisi_list=[]\n",
    "s_length_list = []\n",
    "wave_list = []\n",
    "basic_list = []\n",
    "basic_count_list = []\n",
    "\n",
    "# 'ë³¸ë¬¸'ì— ëŒ€í•œ ì²˜ë¦¬ ì‹œê°„ê³¼ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\n",
    "for i, line in enumerate(data['ë³¸ë¬¸'], start=1): #ğŸ‘ˆë°ì´í„°í”„ë ˆì„ ë³€ê²½ ì‹œ ì—¬ê¸°ì„œ ì„¤ì •!!\n",
    "    res = my_tagger.tags([line], auto_split=True) # ë¬¸ì¥ ê°œìˆ˜\n",
    "    pa = res.pos() # ì¼ìƒ ë¹ˆë„\n",
    "    ma = res.morphs() # ë¬¼ê²° 21\n",
    "    m = res.msg()\n",
    "\n",
    "    #âœ…ì¼ìƒ ë¹ˆë„ (ì˜¨ë¼ì¸)\n",
    "    # 'pa' ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í˜•íƒœì†Œì˜ ì´ ë¹ˆë„ìˆ˜ í•©ì‚°\n",
    "    total_usage_pa = 0\n",
    "\n",
    "    for word, _ in pa:\n",
    "        if word in daily_corp['corpus'].values:\n",
    "            frequency = daily_corp.loc[daily_corp['corpus'] == word, 'frequency'].sum()\n",
    "            total_usage_pa += frequency\n",
    "\n",
    "    #âœ…ê¸°ì´ˆ ì–´íœ˜ ë‚œì´ë„\n",
    "    # ê¸°ì´ˆ ì–´íœ˜ countë¥¼ ìœ„í•œ 'pa' ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í˜•íƒœì†Œì˜ ì´ ë¹ˆë„ìˆ˜ í•©ì‚°\n",
    "    total_usage_basic = 0\n",
    "\n",
    "    for word, pos in pa:\n",
    "        if pos in important_pos_list:\n",
    "            if word in basic_corp['ì–´íœ˜'].values:\n",
    "                match_vocab = basic_corp[basic_corp['ì–´íœ˜'] == word]\n",
    "                if len(match_vocab) == 1:\n",
    "                    rank = sum(match_vocab[\"ë“±ê¸‰\"].values ** 2)\n",
    "                else:\n",
    "                    rank = np.mean(np.array(match_vocab[\"ë“±ê¸‰\"]) ** 2)\n",
    "            else:\n",
    "                rank = 16\n",
    "            count += 1\n",
    "        else:\n",
    "            rank = 0\n",
    "        total_usage_basic += rank\n",
    "    # ê¸°ì´ˆì–´íœ˜ì ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    basic_list.append(total_usage_basic)\n",
    "    basic_count_list.append(count)\n",
    "\n",
    "    #âœ…ì‹ ë¬¸ (ë¬¼ê²°21) ì¶œí˜„ ë¹ˆë„\n",
    "    # 'ma' ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í˜•íƒœì†Œì˜ ì´ ë¹ˆë„ìˆ˜ í•©ì‚°\n",
    "    total_usage_ma = 0\n",
    "    \n",
    "    for morpheme in ma:\n",
    "        if morpheme in wave_corp['ë‹¨ì–´'].values:\n",
    "            frequency = wave_corp.loc[wave_corp['ë‹¨ì–´'] == morpheme, 'ì´ ë¹ˆë„ìˆ˜'].sum()\n",
    "            total_usage_ma += frequency\n",
    "    \n",
    "    # ì¤‘ë³µì„ í¬í•¨í•œ í˜•íƒœì†Œì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ì¼ìƒ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "    word_count = len(pa)\n",
    "    daily_average_usage = total_usage_pa / word_count\n",
    "    # ê³„ì‚°ëœ ì¼ìƒ ë¹ˆë„ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€âœ…\n",
    "    average_usage_list.append(daily_average_usage)\n",
    "    \n",
    "    # ì¤‘ë³µì„ í¬í•¨í•œ í˜•íƒœì†Œì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ë¬¼ê²°21 ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "    morpheme_count = len(ma)\n",
    "    wave_average_usage = total_usage_ma / morpheme_count\n",
    "    # ê³„ì‚°ëœ ë¬¼ê²°21 ë¹ˆë„ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€âœ…\n",
    "    wave_list.append(wave_average_usage)\n",
    "    \n",
    "    # í˜•íƒœì†Œ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€âœ…\n",
    "    count_list.append(word_count)\n",
    "    \n",
    "    #âœ…ìˆ˜ì‹ì–¸ ê°œìˆ˜\n",
    "    filtered_susik = [word for word, pos in pa if pos in susik_set]\n",
    "    count_susik = len(filtered_susik)\n",
    "    # ìˆ˜ì‹ì–¸ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    susik_list.append(count_susik)\n",
    "    \n",
    "    #âœ…ì „ì„±ì–´ë¯¸ ê°œìˆ˜\n",
    "    filtered_junsung = [word for word, pos in pa if pos in junsung_set]\n",
    "    count_junsung = len(filtered_junsung)\n",
    "    # ì „ì„±ì–´ë¯¸ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    junsung_list.append(count_junsung)\n",
    "    \n",
    "    #âœ…ì—°ê²°ì–´ë¯¸ ê°œìˆ˜\n",
    "    filtered_yeongyeol = [word for word, pos in pa if pos == yeongyeol_set]\n",
    "    count_yeongyeol = len(filtered_yeongyeol)\n",
    "    # ì—°ê²°ì–´ë¯¸ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    yeongyeol_list.append(count_yeongyeol)\n",
    "    \n",
    "    #âœ…ì§€ì‹œí‘œí˜„ ê°œìˆ˜\n",
    "    filtered_jisi = [word for word, pos in pa if pos in jisi_set]\n",
    "    count_jisi = len(filtered_jisi)\n",
    "    # ì§€ì‹œí‘œí˜„ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    jisi_list.append(count_jisi)\n",
    "    \n",
    "    #âœ…ë¬¸ì¥ ê°œìˆ˜ë¥¼ count í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    length = len(m.sentences)\n",
    "    s_length_list.append(length)\n",
    "    \n",
    "\n",
    "    # iê°€ 10ì— ë„ë‹¬í•  ë•Œë§ˆë‹¤ ì™„ë£Œ ë©”ì‹œì§€ì™€ ê²½ê³¼ ì‹œê°„ ì¶œë ¥\n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶œë ¥\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì— ì¶”ê°€\n",
    "data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜'] = s_length_list\n",
    "data['ë°”ë¥¸ í˜•íƒœì†Œ ê°œìˆ˜'] = count_list\n",
    "\n",
    "data['ì¼ìƒë¹ˆë„_ì˜¨ë¼ì¸'] = average_usage_list\n",
    "data['ë¬¼ê²°21ë¹ˆë„'] = wave_list\n",
    "data['ë°”ë¥¸ ìˆ˜ì‹ì–¸ ê°œìˆ˜'] = susik_list # NEW\n",
    "data['ë°”ë¥¸ ì „ì„±ì–´ë¯¸ ê°œìˆ˜'] = junsung_list\n",
    "data['ë°”ë¥¸ ì—°ê²°ì–´ë¯¸ ê°œìˆ˜'] = yeongyeol_list\n",
    "data['ë°”ë¥¸ ì§€ì‹œí‘œí˜„ ê°œìˆ˜'] = jisi_list # NEW\n",
    "\n",
    "data['level'] = basic_list # ê¸°ì´ˆì–´íœ˜ ë‚œì´ë„ëŠ” ë‹¤ì‹œ ê°œìˆ˜ë¡œ ë‚˜ëˆ ì¤˜ì•¼ í•¨\n",
    "data['count'] = basic_count_list\n",
    "data['ê¸°ì´ˆì–´íœ˜ë‚œì´ë„'] = data['level']/data['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ2. ìŒìš´ë¡ ì  ë³µì¡ë„ ë³€ìˆ˜\n",
    "- \"ìœ ì•„ í•œê¸€ êµìœ¡ìš© ì–´íœ˜ ëª©ë¡ ì„ ì •ì„ ìœ„í•œ ì—°êµ¬(ìœ¤ê²½ì„ &ì´ìœ ë¯¸, 2014)\"ì˜ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ë°œìŒí•˜ê¸° ì–´ë ¤ìš´ì§€ ì¸¡ì •\n",
    "- ì—°êµ¬ì—ì„œ ì œì‹œëœ ìŒìš´ ì¡°í•© 8ë‹¨ê³„ë¥¼ í•˜ë“œì½”ë”©ìœ¼ë¡œ ì¸¡ì • ê°€ëŠ¥í•˜ë„ë¡ í´ë˜ìŠ¤ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityAnalyzer:\n",
    "    \n",
    "    def __init__(self, already_know=[]):\n",
    "        self.already_know = already_know # í•™ìŠµë‹¨ì–´ ì—°ë™\n",
    "        self.CHO_basic = ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…'] \n",
    "        self.CHO_advanced = ['ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰'] \n",
    "        self.JOONG_single = ['ã…', 'ã…“', 'ã…—', 'ã…œ', 'ã…', 'ã…”', 'ã…¡', 'ã…£'] \n",
    "        self.JOONG_double = ['ã…‘', 'ã…•', 'ã…›', 'ã… ', 'ã…’', 'ã…–', 'ã…˜', 'ã…™', 'ã…š', 'ã…', 'ã…', 'ã…Ÿ', 'ã…¢'] \n",
    "        self.JONG = ['ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']\n",
    "\n",
    "        self.weight_list = [] # ê°€ì¤‘ì¹˜ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "        self.level_weight = None # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "\n",
    "        self.already_words = [] # ë¬¸ì¥ ë‚´ í•™ìŠµ ë‹¨ì–´ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "        \n",
    "        \n",
    "    def syllable_tokenizer(self, s):\n",
    "        temp = s.replace(' ', '').replace('\\n', '').replace('\"', '')\n",
    "        result = []\n",
    "        for c in temp:\n",
    "            result.append(c)\n",
    "        return result\n",
    "    \n",
    "    def normalizer(self, value): # ê°€ì¤‘ì¹˜ ë•Œë¬¸ì— ìµœì € 0.5ê¹Œì§€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ!\n",
    "        return (value - 0.5) / (8 - 1)\n",
    "    \n",
    "    def text_level(self, text):\n",
    "        # ìŒì ˆë‹¨ìœ„ ë¶„ë¦¬\n",
    "        text_syl = self.syllable_tokenizer(text)\n",
    "        \n",
    "        # ìŒì†Œë‹¨ìœ„ ë¶„ë¦¬\n",
    "        pho_list = []\n",
    "        for syl in text_syl:\n",
    "            phoneme = j2hcj(h2j(syl))\n",
    "            pho_list.append(phoneme)\n",
    "        \n",
    "        # complexity íŒë³„\n",
    "        level_list = []\n",
    "        \n",
    "        def jong_no(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single:\n",
    "                level = 1\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single:\n",
    "                level = 2\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double:\n",
    "                level = 5\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double:\n",
    "                level = 7\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        def jong_yes(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 3\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 4\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 6\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 8\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        for pho in pho_list:\n",
    "            if len(pho) == 2:\n",
    "                jong_no(pho)\n",
    "            elif len(pho) == 3:\n",
    "                jong_yes(pho)\n",
    "                \n",
    "        ## [NEW] ê°€ì¤‘ì¹˜ ë°˜ì˜\n",
    "        if self.level_weight is not None : # ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ ìˆëŠ” ê²½ìš°ì—ë§Œ!\n",
    "            #print('level weight', self.level_weight) # ê°€ì¤‘ì¹˜ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ '#'ì„ í•´ì œí•˜ì„¸ìš”\n",
    "            level_list = [a * b for a, b in zip(level_list, self.level_weight)] # ë ˆë²¨ë¦¬ìŠ¤íŠ¸ì— ê°€ì¤‘ì¹˜ë¦¬ìŠ¤íŠ¸ ê³±í•´ì„œ ìµœì¢… ë ˆë²¨ë¦¬ìŠ¤íŠ¸ ìƒì„±!\n",
    "        else :\n",
    "            #print(\"level list:\", level_list) # ë‚œì´ë„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ '#'ì„ í•´ì œí•˜ì„¸ìš”\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if len(level_list) == 0:\n",
    "            #print(text, \"---> ë‚œì´ë„ ë¶ˆí•„ìš” ë¬¸ì¥\") # ë‚œì´ë„ ë¶ˆí•„ìš” ë¬¸ì¥ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ '#'ì„ í•´ì œí•˜ì„¸ìš”\n",
    "            return 0, level_list # ì˜ëª»ëœ ê²½ìš° 0(zero)ì„ ë°˜í™˜ (NaN ê°’ ì²˜ë¦¬)\n",
    "        \n",
    "        syl_len = len(level_list)  # ìŒì ˆ ìˆ˜\n",
    "        level_sum = sum(level_list)  # complexity ì´í•©\n",
    "        score = level_sum / syl_len\n",
    "        \n",
    "        score_scaled = self.normalizer(score)  # ìŠ¤ì¼€ì¼ë§\n",
    "        score_per = round(score_scaled * 100, 1)  # ì ìˆ˜í™”\n",
    "        \n",
    "        # ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œì— í™œìš©í•˜ê¸° ìœ„í•´ level_listë„ í•¨ê»˜ ë°˜í™˜!\n",
    "        return score_per, level_list\n",
    "    \n",
    "    def analyze_syl(self, text): ##ìŒì ˆë‹¨ìœ„ ë¶„ì„ê¸°##\n",
    "        sent_2 = text\n",
    "        score_list1 = []\n",
    "        weight_list = None # ê°€ì¤‘ì¹˜ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "        \n",
    "        score_t = self.text_level(sent_2)\n",
    "\n",
    "        if score_t == 0 :\n",
    "            score_t = 0\n",
    "            score_list1.append(score_t)\n",
    "        else:\n",
    "            score_t = score_t[0] # score ê°’ë§Œ ì‚¬ìš©í•  ê±°ë¼ì„œ ì¸ë±ì‹±\n",
    "            score_list1.append(score_t)\n",
    "                \n",
    "        \n",
    "        df1 = pd.DataFrame({'ë¬¸ì¥ ë‚´ìš©': sent_2, 'ìŒì ˆë‹¨ìœ„ ì ìˆ˜': score_list1})\n",
    "        return df1\n",
    "    \n",
    "    def analyze_noun(self, text): ##ëª…ì‚¬ë‹¨ìœ„ ë¶„ì„ê¸°##\n",
    "        sent_2 = text\n",
    "        score_list2 = []\n",
    "        difficult_list = []  # ì–´ë ¤ìš´ ìŒì ˆ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "        # ëª…ì‚¬ë§Œ ë½‘ì•„ë‚¸ ë’¤ì—,,,\n",
    "        sent_2_noun = []\n",
    "        for t1 in sent_2 :\n",
    "            nouns_2 = komoran.nouns(t1)\n",
    "            sent_2_noun.append(nouns_2)\n",
    "            \n",
    "        # ,,, ë‹¤ì‹œ ì…ë ¥ í˜•íƒœì— ë§ê²Œ ë¬¸ìì—´ë¡œ ì´ì–´ë¶™ì´ê¸°!  \n",
    "        noun_2_str = []\n",
    "        for t2 in sent_2_noun : \n",
    "            \n",
    "            weights, words = self.weight_maker(t2) # ê°€ì¤‘ì¹˜ ë§Œë“œëŠ” í•¨ìˆ˜ë¥¼ ê±°ì¹˜ëŠ” êµ¬ê°„ \n",
    "            self.weight_list.append(weights)\n",
    "            self.already_words.append(words)\n",
    "            \n",
    "            comb_nouns = ' '.join(t2) # ë¬¸ìì—´ ì´ì–´ë¶™ì´ëŠ” êµ¬ê°„\n",
    "            noun_2_str.append(comb_nouns)\n",
    "        \n",
    "        # ê·¸ë¦¬ê³  ê·¸ê±¸ ë‹¤ì‹œ text_levelì— ì…ë ¥!\n",
    "        score_list2 = []\n",
    "\n",
    "        for idx, t3 in enumerate(noun_2_str) : \n",
    "            self.level_weight = self.weight_list[idx] # í•´ë‹¹ ë¬¸ì¥ì— ë§ëŠ” ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ êº¼ë‚´ê¸°\n",
    "            score_t, levels = self.text_level(t3)\n",
    "            if score_t == 0 :\n",
    "                score_t = 0\n",
    "                score_list2.append(score_t)\n",
    "            else:\n",
    "                \n",
    "                score_list2.append(score_t)\n",
    "                \n",
    "                 # [NEW] ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ\n",
    "                \n",
    "            diff_syls = []            \n",
    "            for ldx, level in enumerate(levels):\n",
    "                t3 = t3.replace(' ', '').replace('\\n', '').replace('\"', '') # ê¸¸ì´ ë§ì¶”ê¸° ìœ„í•´ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬í•´ì£¼ê³ \n",
    "                if level >= 5:\n",
    "                    diff_t3 = \"[{}]{}\".format(ldx, t3[ldx])\n",
    "                    diff_syls.append(diff_t3)\n",
    "                else:\n",
    "                    diff_syls.append(' ')\n",
    "            \n",
    "            diff_syls = list(dict.fromkeys(diff_syls)) # ì¤‘ë³µ ì œê±° (setì€ ìˆœì„œê°€ ë’¤ì„ì—¬ì„œ ì‚¬ìš© ì·¨ì†Œ)\n",
    "            diff_syls = '  '.join(diff_syls) # ë³´ê¸° ì¢‹ê²Œ ëŒ€ê´„í˜¸ ì œê±°\n",
    "                    \n",
    "            difficult_list.append(diff_syls) # ë¬¸ì¥ ëŒë©´ì„œ 'ë¬¸ì¥ë³„ ì–´ë ¤ìš´ ìŒì ˆ' ì¶”ê°€\n",
    "            \n",
    "        already_words_set = [', '.join(lst) for lst in self.already_words] # í•™ìŠµí•œ ë‹¨ì–´ë“¤ ì¶œë ¥ (ê¹”ë”í•˜ê²Œ ëŒ€ê´„í˜¸ ì œê±°)\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame({'ë¬¸ì¥ ë‚´ìš©': sent_2, 'ëª…ì‚¬ë‹¨ìœ„ ì ìˆ˜': score_list2, 'ë‚œì´ë„ 5 ì´ìƒ ìŒì ˆ': difficult_list, 'í•™ìŠµí•œ ë‹¨ì–´': already_words_set})\n",
    "        return df2\n",
    "\n",
    "    def weight_maker(self, nounset) : \n",
    "        weight_already = []\n",
    "        word_already = []\n",
    "        \n",
    "        for noun in nounset : \n",
    "\n",
    "            if noun in self.already_know : # ë§Œì•½ ì´ë¯¸ í•™ìŠµí•œ ë‹¨ì–´ë¼ë©´,\n",
    "                word_already.append(noun) # (ã„±) í•™ìŠµí•œ ë‹¨ì–´ ëª©ë¡ì— ì¶”ê°€\n",
    "                for i in range(len(noun)) : # (ã„´) ê°€ì¤‘ì¹˜ ëª©ë¡ì— ìŒì ˆ ìˆ˜ë§Œí¼ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "                    weight_already.append(0.5)\n",
    "            else :\n",
    "                for i in range(len(noun)) :\n",
    "                    weight_already.append(1)\n",
    "        \n",
    "        return weight_already, word_already # ìƒì„±ëœ ê°€ì¤‘ì¹˜ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ì£¼ì!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHO_basic = ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…'] # ê¸°ë³¸ììŒ 10ê°œ\n",
    "CHO_advanced = ['ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰'] # ê²©ìŒ ë˜ëŠ” ê²½ìŒ 9ê°œ\n",
    "\n",
    "JOONG_single = ['ã…', 'ã…“', 'ã…—', 'ã…œ', 'ã…', 'ã…”', 'ã…¡', 'ã…£'] # ë‹¨ëª¨ìŒ 8ê°œ\n",
    "JOONG_double = ['ã…‘', 'ã…•', 'ã…›', 'ã… ', 'ã…’', 'ã…–', 'ã…˜', 'ã…™', 'ã…š', 'ã…', 'ã…', 'ã…Ÿ', 'ã…¢'] # ì´ì¤‘ëª¨ìŒ 13ê°œ\n",
    "\n",
    "JONG = ['ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']\n",
    "\n",
    "\n",
    "def syllable_tokenizer(s):\n",
    "    temp = s.replace(' ', '').replace('\\n', '').replace('\"', '') # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë° ì¤„ë°”ê¿ˆ ì‚­ì œ\n",
    "    result = []\n",
    "    for c in temp: # ì…ë ¥ë°›ì€ tempë¥¼ ì¸ë±ì‹±ìœ¼ë¡œ ëŒë©´ì„œ\n",
    "        result.append(c) # í•œ ìŒì ˆì”© ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    return result\n",
    "\n",
    "def normalizer(value) : # scoreê°’ë“¤ì„ ì •ê·œí™”í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜\n",
    "    return (value - 1) / (8 - 1) # ìµœì†Œ1 ìµœëŒ€8ì˜ ê°’ì„ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##âœ…ìŒìš´ë¡ ì ë³µì¡ë„2 (ìŒì ˆ ìˆ˜ë¡œ ì •ê·œí™”í•˜ì§€ ì•Šì€ ë²„ì „)\n",
    "def text_level(text) :\n",
    "    \n",
    "     # (1) ìŒì ˆë‹¨ìœ„ ë¶„ë¦¬\n",
    "    text_syl = syllable_tokenizer(text)\n",
    "    \n",
    "    # (2) ìŒì†Œë‹¨ìœ„ ë¶„ë¦¬\n",
    "    pho_list=[]\n",
    "    for syl in text_syl :\n",
    "        phoneme = j2hcj(h2j(syl))\n",
    "        pho_list.append(phoneme)\n",
    "    \n",
    "    # (3) ë³µì¡ë„ íŒë³„\n",
    "    level_list = []\n",
    "    # í•¨ìˆ˜ ì„¤ì • (ê¹”ë”í•œ ì¶œë ¥ì„ ìœ„í•´ ë¶ˆí•„ìš”í•œ printë¬¸ ì „ë¶€ ì œê±°í•¨)\n",
    "    def jong_no(word):  \n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single :\n",
    "            level = 1\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single :\n",
    "            level = 4\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double :\n",
    "            level = 25\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double :\n",
    "            level = 49\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    def jong_yes(word):\n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 9\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 16\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 36\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 64\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "    \n",
    "    for pho in pho_list : # (2)ì˜ ê²°ê³¼ pho_listì— ëŒ€í•´ì„œ\n",
    "        if len(pho) == 2 : # ë°›ì¹¨ì´ ì—†ëŠ” ê²½ìš°ëŠ” jong_no í•¨ìˆ˜ë¡œ\n",
    "            jong_no(pho)\n",
    "        elif len(pho) == 3 : # ë°›ì¹¨ì´ ìˆëŠ” ê²½ìš°ëŠ” jong_yes í•¨ìˆ˜ë¡œ\n",
    "            jong_yes(pho)\n",
    "            \n",
    "    #print(level_list) ë ˆë²¨ ë¦¬ìŠ¤íŠ¸ ì™„ì„±\n",
    "    \n",
    "    # ë¬¸ì¥ì´ ì•„ì˜ˆ ë¬¸ì¥ë¶€í˜¸ë¡œë§Œ ì´ë£¨ì–´ì§„ ê²½ìš° ë“± level_listì— ì•„ë¬´ê²ƒë„ ì—†ëŠ” ê²½ìš°ëŠ” ì œì™¸ê°€ í•„ìš”í•¨\n",
    "    if len(level_list)==0 :\n",
    "        print(text, \"---> ë‚œì´ë„ ë¶ˆí•„ìš” ë¬¸ì¥\")\n",
    "        pass\n",
    "    \n",
    "    else :      \n",
    "        # (4) í‰ê· ë³µì¡ë„ ì‚°ì¶œ\n",
    "        syl_len = len(level_list) # ìŒì ˆ ìˆ˜ \n",
    "        #print(syl_len)\n",
    "        level_sum = sum(level_list) # ë³µì¡ë„ ì´í•©\n",
    "        #print(level_sum)\n",
    "        score = level_sum / syl_len\n",
    "\n",
    "        # (5) ìŠ¤ì¼€ì¼ë§ ë° ì ìˆ˜í™”\n",
    "        score_scaled = normalizer(score) # normalizer í•¨ìˆ˜ì— score ì…ë ¥\n",
    "        score_per = round(score_scaled*100, 1) # 100ì„ ê³±í•´ì„œ ì ìˆ˜í™”\n",
    "\n",
    "        # (6) ì •ê·œí™” ì•ˆ í•˜ê³  ê·¸ëƒ¥ ì¶œë ¥!\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì— ì ìš©\n",
    "data['ìŒìš´ë¡ ì ë³µì¡ë„2'] = data['ë³¸ë¬¸'].apply(text_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ3. ìŒì ˆ ê°œìˆ˜, ë‹¨ì–´ ê°œìˆ˜\n",
    "- í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ì—¬ ì¸¡ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ìŒì ˆ ê°œìˆ˜ countí•˜ëŠ” í•¨ìˆ˜\n",
    "def count_characters(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = cleaned_text.split()\n",
    "    total_character_count = sum(len(word) for word in words)\n",
    "    return total_character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì— ì ìš©\n",
    "data['ìŒì ˆ ê°œìˆ˜'] = data['ë³¸ë¬¸'].apply(count_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ë‹¨ì–´ ê°œìˆ˜ countí•˜ëŠ” í•¨ìˆ˜\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì— ì ìš©\n",
    "data['ë‹¨ì–´ ê°œìˆ˜'] = data['ë³¸ë¬¸'].apply(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ4. íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "- í‰ê·  ë¬¸ì¥ ê¸¸ì´ : ë‹¨ì–´ ê°œìˆ˜ / ë¬¸ì¥ ê°œìˆ˜ (='ë¬¸ì¥ ë‹¹ í‰ê·  ë‹¨ì–´ ê°œìˆ˜)\n",
    "- í‰ê·  ë‹¨ì–´ ê¸¸ì´ : ìŒì ˆ ê°œìˆ˜ / ë‹¨ì–´ ê°œìˆ˜ (='ë‹¨ì–´ ë‹¹ í‰ê·  ìŒì ˆ ê°œìˆ˜)\n",
    "- ê¾¸ë°ˆí‘œí˜„ ë“±ì¥ë¹„ìœ¨ : ë°”ë¥¸ ìˆ˜ì‹ì–¸ ê°œìˆ˜ / ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜\n",
    "- ì•ˆì€ë¬¸ì¥ ë“±ì¥ë¹„ìœ¨ : ë°”ë¥¸ ì „ì„±ì–´ë¯¸ ê°œìˆ˜ / ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜\n",
    "- ì´ì–´ì§„ë¬¸ì¥ ë“±ì¥ë¹„ìœ¨ : ë°”ë¥¸ ì—°ê²°ì–´ë¯¸ ê°œìˆ˜ / ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜\n",
    "- ì§€ì‹œí‘œí˜„ ë“±ì¥ë¹„ìœ¨ : ë°”ë¥¸ ì§€ì‹œí‘œí˜„ ê°œìˆ˜ / ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…í‰ê·  ë¬¸ì¥ ê¸¸ì´\n",
    "data['í‰ê·  ë¬¸ì¥ ê¸¸ì´'] = data['ë‹¨ì–´ ê°œìˆ˜'] / data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#âœ…í‰ê·  ë‹¨ì–´ ê¸¸ì´\n",
    "data['í‰ê·  ë‹¨ì–´ ê¸¸ì´'] = data['ìŒì ˆ ê°œìˆ˜'] / data['ë‹¨ì–´ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ê¾¸ë°ˆí‘œí˜„ ë“±ì¥ë¹„ìœ¨\n",
    "data['ê¾¸ë°ˆí‘œí˜„ ë“±ì¥ë¹„ìœ¨'] = data['ë°”ë¥¸ ìˆ˜ì‹ì–¸ ê°œìˆ˜'] / data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ì „ì„±ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨\n",
    "data['ì „ì„±ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨'] = data['ë°”ë¥¸ ì „ì„±ì–´ë¯¸ ê°œìˆ˜'] / data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ì—°ê²°ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨\n",
    "data['ì—°ê²°ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨'] = data['ë°”ë¥¸ ì—°ê²°ì–´ë¯¸ ê°œìˆ˜'] / data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ…ì§€ì‹œí‘œí˜„ ë“±ì¥ë¹„ìœ¨\n",
    "data['ì§€ì‹œí‘œí˜„ ë“±ì¥ë¹„ìœ¨'] = data['ë°”ë¥¸ ì§€ì‹œí‘œí˜„ ê°œìˆ˜'] / data['ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns # í˜„ì¬ ì»¬ëŸ¼ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ5. NF-iDF (News Frequency - inverse Daily Frequency)\n",
    "- ì¼ìƒì—ì„œëŠ” ì˜ ì“°ì´ì§€ ì•ŠëŠ” ì–´íœ˜ì´ë©´ì„œ, ì‹ ë¬¸ì—ì„œ ë§ì´ ì“°ì´ëŠ” ì–´íœ˜ëŠ” ì–´ë µë‹¤ê³  íŒë‹¨ â†’ ì–´ë ¤ìš´ ì–´íœ˜ê°€ ë“±ì¥í•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ â€˜NF-iDFâ€™ë¥¼ ì§ì ‘ ì •ì˜í•¨)\n",
    "- ê³„ì‚° ë°©ì‹ : ë‹¨ì–´ë§ˆë‹¤ {**(ì‹ ë¬¸ê¸°ì‚¬ ì¶œí˜„ë¹ˆë„/ì¼ìƒ ì‚¬ìš©ë¹ˆë„_ì¢…í•©)\\*í•´ë‹¹ ê¸°ì‚¬ ë‚´ ì¶œí˜„ ë¹ˆë„**}ë¥¼ ê³„ì‚° â†’ ê° ê¸°ì‚¬ ë³¸ë¬¸ë§ˆë‹¤ ë‹¨ì–´ ë‚œì´ë„ë¥¼ í‰ê·  ë‚´ì–´ NF-iDFë¡œ í™œìš© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------# 1. ë°”ë¥¸ìœ¼ë¡œ í’ˆì‚¬ íƒœê¹…\n",
    "desired_pos_tags = ['NNG'] # ë‹¨ì–´ì˜ ë‚œì´ë„ ì¸¡ì •ì„ ìœ„í•´ ëª…ì‚¬(NNG)ë§Œ ì·¨ê¸‰\n",
    "\n",
    "start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "\n",
    "score_column = [] # ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "#------------------------------# 2. 'ë¬¼ê²°21ë¹ˆë„/ì¼ìƒë¹ˆë„'ë¥¼ ê³„ì‚°\n",
    "for i, line in enumerate(data['ë³¸ë¬¸'], start=1):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    res = my_tagger.tags([line])\n",
    "\n",
    "    # 'ma' ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í˜•íƒœì†Œì˜ ì´ ë¹ˆë„ìˆ˜ í•©ì‚°\n",
    "    total_usage = 0\n",
    "    \n",
    "    filtered_result = [(word, pos) for word, pos in res.pos() if pos in desired_pos_tags]\n",
    "    tokens = [word for word, _ in filtered_result] # ëª…ì‚¬(NNG) í† í°ë§Œ êº¼ëƒ„\n",
    "    \n",
    "    for token in tokens: \n",
    "        daily_freq = daily_usage.loc[daily_usage['corpus'] == token, 'frequency'].mean() #âœ…ì¼ìƒ ë¹ˆë„ ì¸¡ì •\n",
    "        wave_freq = wave_corp.loc[wave_corp['ë‹¨ì–´'] == token, 'ì´ ë¹ˆë„ìˆ˜'].mean() #âœ…ë¬¼ê²°21 ë¹ˆë„ ì¸¡ì •\n",
    "        if np.isnan(daily_freq):\n",
    "            score = wave_freq / 1 # ë¶„ëª¨ 0 ë°©ì§€\n",
    "        else:\n",
    "            score = wave_freq / daily_freq\n",
    "        \n",
    "        score_tuple = (token, score) # í† í°ê³¼ scoreë¥¼ tuple í˜•íƒœë¡œ ì €ì¥í•˜ê³ \n",
    "        score_list.append(score_tuple) # ë¦¬ìŠ¤íŠ¸ì— í•˜ë‚˜ì”© ì €ì¥\n",
    "    \n",
    "    score_column.append(score_list) # forë¬¸ ë‹¤ ëŒì•„ê°„ ë¦¬ìŠ¤íŠ¸ë“¤ì„ ë‹¤ì‹œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë‚´ì— ì €ì¥\n",
    "    \n",
    "    if i % 10 == 0: # 'ë³¸ë¬¸'ì— ëŒ€í•œ ì²˜ë¦¬ ì‹œê°„ê³¼ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Score Calculating Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "#âœ…ë°ì´í„°ì— ì¶”ê°€\n",
    "data['ì ìˆ˜_tuple'] = score_column\n",
    "    \n",
    "#------------------------------# 3. ë‹¨ì–´ë§ˆë‹¤ 'ê¸°ì‚¬ ë‚´ ì¶œí˜„ ë¹ˆë„'ë¥¼ ë°˜ì˜í•´ë³´ì!\n",
    "total_word_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 2] # 'ì ìˆ˜_tuple' ì—´ì— ì ‘ê·¼\n",
    "    word_list = []\n",
    "    for (word, num) in line: # ì¼ë‹¨ (ë‹¨ì–´,ì ìˆ˜) íŠœí”Œì—ì„œ ë‹¨ì–´ë§Œ ë½‘ì•„ë‚´ê¸°\n",
    "        word_list.append(word)\n",
    "    total_word_list.append(word_list)\n",
    "\n",
    "total_result_list = []\n",
    "\n",
    "for l in total_word_list:\n",
    "    asd = Counter(l)\n",
    "    word_frequency_pairs = list(asd.items()) # Counter í•¨ìˆ˜ë¡œ ë‹¨ì–´ë§ˆë‹¤ ê¸°ì‚¬ ë‚´ì—ì„œ ì–¼ë§ˆë‚˜ ì¶œí˜„í–ˆëŠ”ì§€ ë¹ˆë„ë¥¼ ê³„ì‚°\n",
    "    result_list = []\n",
    "    result_list.extend(word_frequency_pairs)\n",
    "    total_result_list.append(result_list)\n",
    "    \n",
    "# ì¼ë‹¨ counter ê°’ì„ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€ (ë‹¨ì–´ë§ˆë‹¤ í•´ë‹¹ ê¸°ì‚¬ì— ì¶œí˜„í•œ ë¹ˆë„ë¥¼ íŠœí”Œ í˜•íƒœë¡œ ì €ì¥)\n",
    "data['counter'] = total_result_list\n",
    "\n",
    "#------------------------------# 3-1. 'ë¬¼ê²°/ì¼ìƒ' ì ìˆ˜ì™€ 'ê¸°ì‚¬ ë‚´ ì¶œí˜„ ë¹ˆë„'ë¥¼ ê³±í•˜ê¸°\n",
    "result_list = []\n",
    "\n",
    "for i in range(len(data)): # ë°ì´í„°ì…‹ì˜ ëª¨ë“  í–‰ì— ëŒ€í•´\n",
    "    list1 = data.iloc[i, 2] # 'ì ìˆ˜_tuple' ì—´ì— ì ‘ê·¼\n",
    "    list2 = data.iloc[i, 3] # 'counter' ì—´ì— ì ‘ê·¼\n",
    "    \n",
    "    new_tuple_list = []\n",
    "\n",
    "    # ì²«ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë£¨í”„\n",
    "    for tup1 in list1:        \n",
    "        # íŠœí”Œì—ì„œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        word = tup1[0]\n",
    "\n",
    "        # ë‘ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë‹¨ì–´ì— ëŒ€í•œ íŠœí”Œ ì°¾ê¸°\n",
    "        matching_tup2 = next((tup for tup in list2 if tup[0] == word), None)\n",
    "\n",
    "        # ë§Œì•½ì— ì°¾ì€ ê²½ìš°\n",
    "        if matching_tup2 is not None:\n",
    "            # nanê³¼ ìˆ«ìë¥¼ ê³±í•´ ìƒˆë¡œìš´ íŠœí”Œ ìƒì„±\n",
    "            new_tuple = (word, matching_tup2[1] * tup1[1])\n",
    "            new_tuple_list.append(new_tuple)\n",
    "            \n",
    "    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ìƒˆ íŠœí”Œë¦¬ìŠ¤íŠ¸ë“¤ì„ ì¶•ì !\n",
    "    result_list.append(new_tuple_list)\n",
    "    \n",
    "#âœ…ë°ì´í„°ì— ì¶”ê°€\n",
    "data['ë‹¨ì–´ë‚œì´ë„'] = result_list\n",
    "\n",
    "#------------------------------# 4. (ë‹¨ì–´, ë‹¨ì–´ ë‚œì´ë„) ìŒì— ëŒ€í•œ ì „ì²˜ë¦¬ : nan ì œê±°, ì¤‘ë³µ ì œê±°, ë‚œì´ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬, ë¶ˆìš©ì–´ ì œê±°\n",
    "stopwords = ['ìš”êµ¬', 'íŠ¹íŒŒì›', 'ì°¸ì„ì', 'í¬ìƒì', 'ê¸°ì', 'ì§€ë‚œí•´', 'ì–‘êµ­', 'ê°‘', 'ì„', 'ì¤‘ëŒ€í˜•', 'ìŠ¹ìš©ì°¨', 'ì´ë“¬í•´', 'í•¸ë“œë³¼',\n",
    "             'êµ­ê°€', 'ì—°í•©ë‰´ìŠ¤', 'ë‹¹êµ­', 'ì§€ë‚œí•´', 'ê¸°ì—…', 'ìƒìŠ¹ì„¸', 'ë‹·ìƒˆ', 'ëˆ„ë¦¬ì§‘', 'ê¼´ì°Œ', 'ì‚¬ë§ì', 'ì´ë‚ ', 'ëŒ€í†µë ¹', 'ì§€ì—­',\n",
    "             'ì‹œì¸', 'ë©”ì‹œì§€', 'ì„¼í„°', 'ì‹œ', 'ì˜ë£Œì›', 'ë¶•ê´´', 'ê¸°ìì‹¤', 'ë³´ê³ ì„œ', 'ì†Œí­', 'ë¼ì´ë²Œ', 'ë…¸ì¡°', 'ë‚´ë…„ë„', 'ê²¬ì œ', 'ì•µì»¤',\n",
    "             'ë…¼ì„¤ìœ„ì›', 'ìˆ˜ë½', 'ë¦¬ì„œì¹˜', 'íƒ€ì„ìŠ¤', 'ë¬´ì£„', 'ë‰´ì‹œìŠ¤', 'ë„', 'ì¡°ì‚¬', 'ìƒë‹¹ìˆ˜', 'ì§€ë‚œë‹¬', 'ë§ˆë‹¤', 'ì£¼', 'ê°€ìš´ë°', 'ê°œë°©',\n",
    "            'ì—°í‰ê· ', 'ê³ ì†ë²„ìŠ¤', 'í‰ê· ', 'ê´€ê³„ì', 'ê³ êµ', 'ì—°ë©´ì ', 'ì°¸ê°€ì', 'ë‹¹ì‹œ'] \n",
    "\n",
    "final_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]  # 'ë‹¨ì–´ë‚œì´ë„' ì—´ì— ì ‘ê·¼\n",
    "    line_without_nan = [(word, num) for word, num in line if not pd.isna(num)]  # tuple ë‚´ì— nan ìˆìœ¼ë©´ ì œê±°\n",
    "    sorted_result = sorted(line_without_nan, key=lambda x: x[1], reverse=True)  # ìµœì¢… ë‚œì´ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    unique_values = set()\n",
    "    unique_result = [(word, num) for word, num in sorted_result if word not in unique_values and not unique_values.add(word)]\n",
    "    # ë¶ˆìš©ì–´ ì œê±°\n",
    "    unique_result = [(word, num) for word, num in unique_result if word not in stopwords]\n",
    "\n",
    "    final_list.append(unique_result)\n",
    "\n",
    "#âœ…ë°ì´í„°ì— ì „ì²˜ë¦¬í•œ ê°’ìœ¼ë¡œ ê°±ì‹  (ì—¬ì „íˆ íŠœí”Œ í˜•íƒœ)\n",
    "data['ë‹¨ì–´ë‚œì´ë„'] = final_list\n",
    "\n",
    "#------------------------------# 5. ìµœì¢… NF-iDFë¥¼ ê³„ì‚° (ê°œë³„ ë‹¨ì–´ë§ˆë‹¤ ì¸¡ì •ëœ ë‚œì´ë„ë“¤ì˜ í‰ê· ì„ ê³„ì‚°)\n",
    "NFiDF_result = []\n",
    "\n",
    "for i in range(len(data)): # ë°ì´í„°ì…‹ì˜ ëª¨ë“  í–‰ì— ëŒ€í•´\n",
    "    list1 = data.iloc[i, 4] # 'ë‹¨ì–´ë‚œì´ë„' ì—´ì— ì ‘ê·¼\n",
    "                           \n",
    "    ë‹¨ì–´ë‚œì´ë„_list = []\n",
    "\n",
    "    for tup1 in list1:        \n",
    "        # íŠœí”Œì—ì„œ ìˆ«ì(ë‹¨ì–´ë³„ë¡œ ì¸¡ì •ëœ ë‚œì´ë„) ì¶”ì¶œ\n",
    "        ë‹¨ì–´ë‚œì´ë„ = tup1[1]\n",
    "        ë‹¨ì–´ë‚œì´ë„_list.append(ë‹¨ì–´ë‚œì´ë„)\n",
    "            \n",
    "    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ìƒˆ íŠœí”Œë¦¬ìŠ¤íŠ¸ë“¤ì„ ì¶•ì !\n",
    "    ë‹¨ì–´ë‚œì´ë„_sum = sum(ë‹¨ì–´ë‚œì´ë„_list)/len(ë‹¨ì–´ë‚œì´ë„_list)\n",
    "    NFiDF_result.append(ë‹¨ì–´ë‚œì´ë„_sum)\n",
    "\n",
    "#âœ…ë°ì´í„°ì— ì¶”ê°€ â¡ï¸ ìµœì¢… NF-iDF ê³„ì‚° ì™„ë£Œ !!!\n",
    "data['ëª…ì‚¬ë‚œì´ë„ í‰ê·  ì ìˆ˜'] = NFiDF_result\n",
    "\n",
    "\n",
    "#------------------------------#ğŸ“Œì¶”ê°€ ì‘ì—…\n",
    "#------------------------------# NF-iDFë¥¼ í™œìš©í•´ì„œ ë‹¨ì–´ì¥ìœ¼ë¡œ ì œê³µí•  ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ì!\n",
    "num_list = [] # ìµœì¢…ë‚œì´ë„ë§Œ ë½‘ì•„ë‚´ê¸°\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4] # 'ë‹¨ì–´ë‚œì´ë„' ì—´ì— ì ‘ê·¼\n",
    "    for (_, num) in line:\n",
    "        num_list.append(num)\n",
    "\n",
    "top6_list = [] # ìµœì¢…ë‚œì´ë„ ìƒìœ„ 6ê°œë§Œ ì¶”ì¶œ â†’ GPT ê¸°ë°˜ ë‹¨ì–´ ì„¤ëª… ì˜ˆì •\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]\n",
    "    lbyl_list = []\n",
    "    for (word, num) in line:\n",
    "        lbyl_list.append(word)\n",
    "        if len(lbyl_list) == 6:\n",
    "            break\n",
    "    top6_list.append(lbyl_list)\n",
    "\n",
    "#âœ…ë°ì´í„°ì— ì¶”ê°€ (=ë‹¨ì–´ì¥ ì œê³µí•  ë‹¨ì–´!!)\n",
    "data['top6'] = top6_list\n",
    "\n",
    "\n",
    "# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶œë ¥\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Final Score Calculated elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ6. êµ­ë¦½êµ­ì–´ì› ë¹ˆë„ ë°ì´í„° ì¶”ê°€\n",
    "- êµ­ë¦½êµ­ì–´ì›ì˜ 'í˜„ëŒ€ êµ­ì–´ ì‚¬ìš© ë¹ˆë„ ì¡°ì‚¬(2015)' xls íŒŒì¼ì„ ë°œê²¬í•˜ì—¬ ì¶”ê°€\n",
    "- ì¼ìƒë¹ˆë„ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì¼ìƒë¹ˆë„(ì˜¨ë¼ì¸) ë³€ìˆ˜ì™€ êµ­ë¦½êµ­ì–´ì› ê¸°ë°˜ ì¼ìƒë¹ˆë„ ë³€ìˆ˜ë¥¼ í•©ì³ì„œ ì‚¬ìš©í•˜ê¸°ë¡œ í•¨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('~~í˜„ëŒ€ êµ­ì–´ ì‚¬ìš© ë¹ˆë„ ì¡°ì‚¬ ê²°ê³¼~~.csv', encoding='utf-8-sig')\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•ì„œ ì‚¬ìš©í•œ ì½”ë“œ(`ì¼ìƒë¹ˆë„_ì˜¨ë¼ì¸`)ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¬ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = time.time()\n",
    "\n",
    "# ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "average_usage_list = []\n",
    "count_list = []\n",
    "susik_list = []\n",
    "junsung_list = []\n",
    "yeongyeol_list = []\n",
    "jisi_list=[]\n",
    "s_length_list = []\n",
    "wave_list = []\n",
    "basic_list = []\n",
    "basic_count_list =[]\n",
    "\n",
    "# 'ë³¸ë¬¸'ì— ëŒ€í•œ ì²˜ë¦¬ ì‹œê°„ê³¼ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\n",
    "for i, line in enumerate(data9181['ë³¸ë¬¸'], start=1): \n",
    "    res = my_tagger.tags([line])\n",
    "    pa = res.pos() # ì¼ìƒ ë¹ˆë„\n",
    "    ma = res.morphs() # ë¬¼ê²° 21\n",
    "    \n",
    "#     res = my_tagger.tags([line], auto_split=True) # ë¬¸ì¥ ê°œìˆ˜\n",
    "#     m = res.msg()\n",
    "\n",
    "    #âœ…ì¼ìƒ ë¹ˆë„ (êµ­ë¦½êµ­ì–´ì›)\n",
    "    # 'pa' ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í˜•íƒœì†Œì˜ ì´ ë¹ˆë„ìˆ˜ í•©ì‚°\n",
    "    total_usage_pa = 0\n",
    "\n",
    "    for word, _ in pa:\n",
    "        if word in df1['í•­ëª©'].values:\n",
    "            frequency = df1.loc[df1['í•­ëª©'] == word, 'ë¹ˆë„'].sum()\n",
    "            total_usage_pa += frequency\n",
    "\n",
    "     # ì¤‘ë³µì„ í¬í•¨í•œ í˜•íƒœì†Œì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ì¼ìƒ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "    word_count = len(pa)\n",
    "    daily_average_usage = total_usage_pa / word_count\n",
    "     # ê³„ì‚°ëœ ì¼ìƒ ë¹ˆë„ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    average_usage_list.append(daily_average_usage)\n",
    "    \n",
    "    # iê°€ 10ì— ë„ë‹¬í•  ë•Œë§ˆë‹¤ ì™„ë£Œ ë©”ì‹œì§€ì™€ ê²½ê³¼ ì‹œê°„ ì¶œë ¥\n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶œë ¥\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ë°ì´í„°ì— ì¶”ê°€ (df_new_9520)\n",
    "data['ì¼ìƒë¹ˆë„_êµ­ë¦½êµ­ì–´ì›'] = average_usage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¬ëŸ¼ ìˆœì„œ ê¹”ë”í•˜ê²Œ ì •ë¦¬.\n",
    "df_new_9520 = data[['ì–¸ë¡ ì‚¬', 'ë³¸ë¬¸', 'ìŒì ˆ ê°œìˆ˜', 'ë‹¨ì–´ ê°œìˆ˜', 'ë°”ë¥¸ í˜•íƒœì†Œ ê°œìˆ˜', 'ë°”ë¥¸ ê¾¸ë°ˆí‘œí˜„ ê°œìˆ˜', 'ë°”ë¥¸ ì§€ì‹œí‘œí˜„ ê°œìˆ˜',\n",
    "       'ë°”ë¥¸ ì „ì„±ì–´ë¯¸ ê°œìˆ˜', 'ë°”ë¥¸ ì—°ê²°ì–´ë¯¸ ê°œìˆ˜', 'ë°”ë¥¸ ë¬¸ì¥ ê°œìˆ˜', 'ìŒìš´ë¡ ì ë³µì¡ë„2', 'ì¼ìƒë¹ˆë„_ì˜¨ë¼ì¸', 'ì¼ìƒë¹ˆë„_êµ­ë¦½êµ­ì–´ì›',\n",
    "       'ë¬¼ê²°21ë¹ˆë„', 'ê¸°ì´ˆì–´íœ˜ ë‚œì´ë„', 'í‰ê·  ë¬¸ì¥ ê¸¸ì´', 'í‰ê·  ë‹¨ì–´ ê¸¸ì´', 'ê¾¸ë°ˆí‘œí˜„ ë“±ì¥ë¹„ìœ¨', 'ì§€ì‹œí‘œí˜„ ë“±ì¥ë¹„ìœ¨',\n",
    "       'ì „ì„±ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨', 'ì—°ê²°ì–´ë¯¸ ë“±ì¥ë¹„ìœ¨']]\n",
    "df_new_9520"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
