{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔖목차\n",
    "**1. 바른 형태소 분석기로 측정한 변수**\n",
    "- 문장 개수, 형태소 개수\n",
    "- 일상 출현 빈도(온라인), 신문 출현 빈도(물결21), 기초 어휘 난이도\n",
    "- 수식언 개수, 전성어미 개수, 연결어미 개수, 지시표현 개수\n",
    "\n",
    "**2. 음운론적 복잡도 변수**\n",
    "\n",
    "**3. 음절 개수, 단어 개수**\n",
    "\n",
    "**4. 파생변수 생성**\n",
    "- 평균 문장 길이, 평균 단어 길이\n",
    "- 꾸밈표현 등장비율, 안은문장 등장비율, 이어진문장 등장비율, 지시표현 등장비율\n",
    "\n",
    "**5. NF-iDF (News Frequenct - inverse Daily Frequency)**\n",
    "\n",
    "**6. 일상 출현 빈도(국립국어원)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import bareunpy\n",
    "from bareunpy import Tagger\n",
    "\n",
    "from jamo import h2j, j2hcj\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌1. 바른 형태소 분석기로 측정한 변수\n",
    "- 문장 개수, 형태소 개수\n",
    "- 일상 출현 빈도, 신문 출현 빈도(물결21), 기초 어휘 난이도\n",
    "- 수식언 개수, 전성어미 개수, 연결어미 개수, 지시표현 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바른 API 준비 \n",
    "API_KEY=\"본인의 API Key 입력\" \n",
    "my_tagger = Tagger(API_KEY, 'localhost') # Tagger 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⭐신문 기사 전체 데이터셋 (10년치 취합본)\n",
    "data = pd.read_csv('~~신문 기사 전체 데이터셋~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 일상빈도 데이터\n",
    "daily_corp = pd.read_csv('~~일상빈도 데이터~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 물결21 빈도 데이터\n",
    "wave_corp = pd.read_csv('~~물결21 빈도 데이터~~.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 국립국어원 기초어휘 데이터\n",
    "basic_corp = pd.read_csv('~~국립국어원 기초어휘 데이터~~.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신문기사 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 신문 기사 데이터셋에서 필요한 부분만 추출\n",
    "data = data8565[['언론사', '본문']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 생성에 사용할 데이터(일상빈도, 물결21, 국립국어원 기초어휘)도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일상빈도 데이터에서 필요한 부분만 추출\n",
    "daily_corp = daily_corp[daily_corp['frequency'] >= np.mean(daily_corp['frequency'])]\n",
    "daily_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 물결21 데이터에서 필요한 부분만 추출\n",
    "wave_corp = wave_corp[['태그', '단어', '총 빈도수']]\n",
    "wave_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초어휘 난이도는 그대로 사용\n",
    "basic_corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[본문] 컬럼의 텍스트에 loop 돌면서 필요한 변수들 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 품사 태그 지정 -> 수식언, 연결어미, 전성어미, 지시표현\n",
    "\n",
    "susik_set = ['MMA', 'MAG'] # 접속부사 MAJ 제외\n",
    "junsung_set = ['ETN', 'ETM'] # 전성어미\n",
    "yeongyeol_set = 'EC' # 연결미미\n",
    "jisi_set = ['NP', 'MMD', 'MAJ'] # 지시표현\n",
    "important_pos_list = [\"NNG\", 'NNP', \"NNB\", \"NP\", \"NR\", \"NF\", \"NA\", \"NV\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"MMA\", \"MMD\", \"MMN\", \"MAG\", \"MAJ\"] # 기초어휘 품사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 값을 저장할 리스트 \n",
    "average_usage_list = []\n",
    "count_list = []\n",
    "susik_list = []\n",
    "junsung_list = []\n",
    "yeongyeol_list = []\n",
    "jisi_list=[]\n",
    "s_length_list = []\n",
    "wave_list = []\n",
    "basic_list = []\n",
    "basic_count_list = []\n",
    "\n",
    "# '본문'에 대한 처리 시간과 진행 상황을 모니터링\n",
    "for i, line in enumerate(data['본문'], start=1): #👈데이터프레임 변경 시 여기서 설정!!\n",
    "    res = my_tagger.tags([line], auto_split=True) # 문장 개수\n",
    "    pa = res.pos() # 일상 빈도\n",
    "    ma = res.morphs() # 물결 21\n",
    "    m = res.msg()\n",
    "\n",
    "    #✅일상 빈도 (온라인)\n",
    "    # 'pa' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_pa = 0\n",
    "\n",
    "    for word, _ in pa:\n",
    "        if word in daily_corp['corpus'].values:\n",
    "            frequency = daily_corp.loc[daily_corp['corpus'] == word, 'frequency'].sum()\n",
    "            total_usage_pa += frequency\n",
    "\n",
    "    #✅기초 어휘 난이도\n",
    "    # 기초 어휘 count를 위한 'pa' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_basic = 0\n",
    "\n",
    "    for word, pos in pa:\n",
    "        if pos in important_pos_list:\n",
    "            if word in basic_corp['어휘'].values:\n",
    "                match_vocab = basic_corp[basic_corp['어휘'] == word]\n",
    "                if len(match_vocab) == 1:\n",
    "                    rank = sum(match_vocab[\"등급\"].values ** 2)\n",
    "                else:\n",
    "                    rank = np.mean(np.array(match_vocab[\"등급\"]) ** 2)\n",
    "            else:\n",
    "                rank = 16\n",
    "            count += 1\n",
    "        else:\n",
    "            rank = 0\n",
    "        total_usage_basic += rank\n",
    "    # 기초어휘점수를 리스트에 추가\n",
    "    basic_list.append(total_usage_basic)\n",
    "    basic_count_list.append(count)\n",
    "\n",
    "    #✅신문 (물결21) 출현 빈도\n",
    "    # 'ma' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_ma = 0\n",
    "    \n",
    "    for morpheme in ma:\n",
    "        if morpheme in wave_corp['단어'].values:\n",
    "            frequency = wave_corp.loc[wave_corp['단어'] == morpheme, '총 빈도수'].sum()\n",
    "            total_usage_ma += frequency\n",
    "    \n",
    "    # 중복을 포함한 형태소의 개수로 나누어 평균 일상 빈도수 계산\n",
    "    word_count = len(pa)\n",
    "    daily_average_usage = total_usage_pa / word_count\n",
    "    # 계산된 일상 빈도를 리스트에 추가✅\n",
    "    average_usage_list.append(daily_average_usage)\n",
    "    \n",
    "    # 중복을 포함한 형태소의 개수로 나누어 평균 물결21 빈도수 계산\n",
    "    morpheme_count = len(ma)\n",
    "    wave_average_usage = total_usage_ma / morpheme_count\n",
    "    # 계산된 물결21 빈도수를 리스트에 추가✅\n",
    "    wave_list.append(wave_average_usage)\n",
    "    \n",
    "    # 형태소 개수를 리스트에 추가✅\n",
    "    count_list.append(word_count)\n",
    "    \n",
    "    #✅수식언 개수\n",
    "    filtered_susik = [word for word, pos in pa if pos in susik_set]\n",
    "    count_susik = len(filtered_susik)\n",
    "    # 수식언 개수를 리스트에 추가\n",
    "    susik_list.append(count_susik)\n",
    "    \n",
    "    #✅전성어미 개수\n",
    "    filtered_junsung = [word for word, pos in pa if pos in junsung_set]\n",
    "    count_junsung = len(filtered_junsung)\n",
    "    # 전성어미 개수를 리스트에 추가\n",
    "    junsung_list.append(count_junsung)\n",
    "    \n",
    "    #✅연결어미 개수\n",
    "    filtered_yeongyeol = [word for word, pos in pa if pos == yeongyeol_set]\n",
    "    count_yeongyeol = len(filtered_yeongyeol)\n",
    "    # 연결어미 개수를 리스트에 추가\n",
    "    yeongyeol_list.append(count_yeongyeol)\n",
    "    \n",
    "    #✅지시표현 개수\n",
    "    filtered_jisi = [word for word, pos in pa if pos in jisi_set]\n",
    "    count_jisi = len(filtered_jisi)\n",
    "    # 지시표현 개수를 리스트에 추가\n",
    "    jisi_list.append(count_jisi)\n",
    "    \n",
    "    #✅문장 개수를 count 후 리스트에 추가\n",
    "    length = len(m.sentences)\n",
    "    s_length_list.append(length)\n",
    "    \n",
    "\n",
    "    # i가 10에 도달할 때마다 완료 메시지와 경과 시간 출력\n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 추가\n",
    "data['바른 문장 개수'] = s_length_list\n",
    "data['바른 형태소 개수'] = count_list\n",
    "\n",
    "data['일상빈도_온라인'] = average_usage_list\n",
    "data['물결21빈도'] = wave_list\n",
    "data['바른 수식언 개수'] = susik_list # NEW\n",
    "data['바른 전성어미 개수'] = junsung_list\n",
    "data['바른 연결어미 개수'] = yeongyeol_list\n",
    "data['바른 지시표현 개수'] = jisi_list # NEW\n",
    "\n",
    "data['level'] = basic_list # 기초어휘 난이도는 다시 개수로 나눠줘야 함\n",
    "data['count'] = basic_count_list\n",
    "data['기초어휘난이도'] = data['level']/data['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌2. 음운론적 복잡도 변수\n",
    "- \"유아 한글 교육용 어휘 목록 선정을 위한 연구(윤경선&이유미, 2014)\"의 내용을 활용하여 텍스트가 얼마나 발음하기 어려운지 측정\n",
    "- 연구에서 제시된 음운 조합 8단계를 하드코딩으로 측정 가능하도록 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityAnalyzer:\n",
    "    \n",
    "    def __init__(self, already_know=[]):\n",
    "        self.already_know = already_know # 학습단어 연동\n",
    "        self.CHO_basic = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅎ'] \n",
    "        self.CHO_advanced = ['ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ'] \n",
    "        self.JOONG_single = ['ㅏ', 'ㅓ', 'ㅗ', 'ㅜ', 'ㅐ', 'ㅔ', 'ㅡ', 'ㅣ'] \n",
    "        self.JOONG_double = ['ㅑ', 'ㅕ', 'ㅛ', 'ㅠ', 'ㅒ', 'ㅖ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅞ', 'ㅝ', 'ㅟ', 'ㅢ'] \n",
    "        self.JONG = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "        self.weight_list = [] # 가중치 담을 리스트 준비\n",
    "        self.level_weight = None # 가중치 초기화\n",
    "\n",
    "        self.already_words = [] # 문장 내 학습 단어 담을 리스트 준비\n",
    "        \n",
    "        \n",
    "    def syllable_tokenizer(self, s):\n",
    "        temp = s.replace(' ', '').replace('\\n', '').replace('\"', '')\n",
    "        result = []\n",
    "        for c in temp:\n",
    "            result.append(c)\n",
    "        return result\n",
    "    \n",
    "    def normalizer(self, value): # 가중치 때문에 최저 0.5까지 나올 수 있음!\n",
    "        return (value - 0.5) / (8 - 1)\n",
    "    \n",
    "    def text_level(self, text):\n",
    "        # 음절단위 분리\n",
    "        text_syl = self.syllable_tokenizer(text)\n",
    "        \n",
    "        # 음소단위 분리\n",
    "        pho_list = []\n",
    "        for syl in text_syl:\n",
    "            phoneme = j2hcj(h2j(syl))\n",
    "            pho_list.append(phoneme)\n",
    "        \n",
    "        # complexity 판별\n",
    "        level_list = []\n",
    "        \n",
    "        def jong_no(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single:\n",
    "                level = 1\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single:\n",
    "                level = 2\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double:\n",
    "                level = 5\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double:\n",
    "                level = 7\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        def jong_yes(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 3\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 4\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 6\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 8\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        for pho in pho_list:\n",
    "            if len(pho) == 2:\n",
    "                jong_no(pho)\n",
    "            elif len(pho) == 3:\n",
    "                jong_yes(pho)\n",
    "                \n",
    "        ## [NEW] 가중치 반영\n",
    "        if self.level_weight is not None : # 가중치 리스트 있는 경우에만!\n",
    "            #print('level weight', self.level_weight) # 가중치 확인하고 싶으면 '#'을 해제하세요\n",
    "            level_list = [a * b for a, b in zip(level_list, self.level_weight)] # 레벨리스트에 가중치리스트 곱해서 최종 레벨리스트 생성!\n",
    "        else :\n",
    "            #print(\"level list:\", level_list) # 난이도 확인하고 싶으면 '#'을 해제하세요\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if len(level_list) == 0:\n",
    "            #print(text, \"---> 난이도 불필요 문장\") # 난이도 불필요 문장을 확인하고 싶으면 '#'을 해제하세요\n",
    "            return 0, level_list # 잘못된 경우 0(zero)을 반환 (NaN 값 처리)\n",
    "        \n",
    "        syl_len = len(level_list)  # 음절 수\n",
    "        level_sum = sum(level_list)  # complexity 총합\n",
    "        score = level_sum / syl_len\n",
    "        \n",
    "        score_scaled = self.normalizer(score)  # 스케일링\n",
    "        score_per = round(score_scaled * 100, 1)  # 점수화\n",
    "        \n",
    "        # 어려운 단어 추출에 활용하기 위해 level_list도 함께 반환!\n",
    "        return score_per, level_list\n",
    "    \n",
    "    def analyze_syl(self, text): ##음절단위 분석기##\n",
    "        sent_2 = text\n",
    "        score_list1 = []\n",
    "        weight_list = None # 가중치리스트 초기화\n",
    "        \n",
    "        score_t = self.text_level(sent_2)\n",
    "\n",
    "        if score_t == 0 :\n",
    "            score_t = 0\n",
    "            score_list1.append(score_t)\n",
    "        else:\n",
    "            score_t = score_t[0] # score 값만 사용할 거라서 인덱싱\n",
    "            score_list1.append(score_t)\n",
    "                \n",
    "        \n",
    "        df1 = pd.DataFrame({'문장 내용': sent_2, '음절단위 점수': score_list1})\n",
    "        return df1\n",
    "    \n",
    "    def analyze_noun(self, text): ##명사단위 분석기##\n",
    "        sent_2 = text\n",
    "        score_list2 = []\n",
    "        difficult_list = []  # 어려운 음절 저장할 리스트\n",
    "        \n",
    "        # 명사만 뽑아낸 뒤에,,,\n",
    "        sent_2_noun = []\n",
    "        for t1 in sent_2 :\n",
    "            nouns_2 = komoran.nouns(t1)\n",
    "            sent_2_noun.append(nouns_2)\n",
    "            \n",
    "        # ,,, 다시 입력 형태에 맞게 문자열로 이어붙이기!  \n",
    "        noun_2_str = []\n",
    "        for t2 in sent_2_noun : \n",
    "            \n",
    "            weights, words = self.weight_maker(t2) # 가중치 만드는 함수를 거치는 구간 \n",
    "            self.weight_list.append(weights)\n",
    "            self.already_words.append(words)\n",
    "            \n",
    "            comb_nouns = ' '.join(t2) # 문자열 이어붙이는 구간\n",
    "            noun_2_str.append(comb_nouns)\n",
    "        \n",
    "        # 그리고 그걸 다시 text_level에 입력!\n",
    "        score_list2 = []\n",
    "\n",
    "        for idx, t3 in enumerate(noun_2_str) : \n",
    "            self.level_weight = self.weight_list[idx] # 해당 문장에 맞는 가중치 리스트 꺼내기\n",
    "            score_t, levels = self.text_level(t3)\n",
    "            if score_t == 0 :\n",
    "                score_t = 0\n",
    "                score_list2.append(score_t)\n",
    "            else:\n",
    "                \n",
    "                score_list2.append(score_t)\n",
    "                \n",
    "                 # [NEW] 어려운 단어 추출\n",
    "                \n",
    "            diff_syls = []            \n",
    "            for ldx, level in enumerate(levels):\n",
    "                t3 = t3.replace(' ', '').replace('\\n', '').replace('\"', '') # 길이 맞추기 위해 동일하게 전처리해주고\n",
    "                if level >= 5:\n",
    "                    diff_t3 = \"[{}]{}\".format(ldx, t3[ldx])\n",
    "                    diff_syls.append(diff_t3)\n",
    "                else:\n",
    "                    diff_syls.append(' ')\n",
    "            \n",
    "            diff_syls = list(dict.fromkeys(diff_syls)) # 중복 제거 (set은 순서가 뒤섞여서 사용 취소)\n",
    "            diff_syls = '  '.join(diff_syls) # 보기 좋게 대괄호 제거\n",
    "                    \n",
    "            difficult_list.append(diff_syls) # 문장 돌면서 '문장별 어려운 음절' 추가\n",
    "            \n",
    "        already_words_set = [', '.join(lst) for lst in self.already_words] # 학습한 단어들 출력 (깔끔하게 대괄호 제거)\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame({'문장 내용': sent_2, '명사단위 점수': score_list2, '난이도 5 이상 음절': difficult_list, '학습한 단어': already_words_set})\n",
    "        return df2\n",
    "\n",
    "    def weight_maker(self, nounset) : \n",
    "        weight_already = []\n",
    "        word_already = []\n",
    "        \n",
    "        for noun in nounset : \n",
    "\n",
    "            if noun in self.already_know : # 만약 이미 학습한 단어라면,\n",
    "                word_already.append(noun) # (ㄱ) 학습한 단어 목록에 추가\n",
    "                for i in range(len(noun)) : # (ㄴ) 가중치 목록에 음절 수만큼 가중치 추가\n",
    "                    weight_already.append(0.5)\n",
    "            else :\n",
    "                for i in range(len(noun)) :\n",
    "                    weight_already.append(1)\n",
    "        \n",
    "        return weight_already, word_already # 생성된 가중치리스트를 반환해주자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHO_basic = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅎ'] # 기본자음 10개\n",
    "CHO_advanced = ['ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ'] # 격음 또는 경음 9개\n",
    "\n",
    "JOONG_single = ['ㅏ', 'ㅓ', 'ㅗ', 'ㅜ', 'ㅐ', 'ㅔ', 'ㅡ', 'ㅣ'] # 단모음 8개\n",
    "JOONG_double = ['ㅑ', 'ㅕ', 'ㅛ', 'ㅠ', 'ㅒ', 'ㅖ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅞ', 'ㅝ', 'ㅟ', 'ㅢ'] # 이중모음 13개\n",
    "\n",
    "JONG = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "def syllable_tokenizer(s):\n",
    "    temp = s.replace(' ', '').replace('\\n', '').replace('\"', '') # 불필요한 기호 및 줄바꿈 삭제\n",
    "    result = []\n",
    "    for c in temp: # 입력받은 temp를 인덱싱으로 돌면서\n",
    "        result.append(c) # 한 음절씩 결과 리스트에 추가\n",
    "    return result\n",
    "\n",
    "def normalizer(value) : # score값들을 정규화해주는 함수를 정의\n",
    "    return (value - 1) / (8 - 1) # 최소1 최대8의 값을 0~1 사이로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##✅음운론적복잡도2 (음절 수로 정규화하지 않은 버전)\n",
    "def text_level(text) :\n",
    "    \n",
    "     # (1) 음절단위 분리\n",
    "    text_syl = syllable_tokenizer(text)\n",
    "    \n",
    "    # (2) 음소단위 분리\n",
    "    pho_list=[]\n",
    "    for syl in text_syl :\n",
    "        phoneme = j2hcj(h2j(syl))\n",
    "        pho_list.append(phoneme)\n",
    "    \n",
    "    # (3) 복잡도 판별\n",
    "    level_list = []\n",
    "    # 함수 설정 (깔끔한 출력을 위해 불필요한 print문 전부 제거함)\n",
    "    def jong_no(word):  \n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single :\n",
    "            level = 1\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single :\n",
    "            level = 4\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double :\n",
    "            level = 25\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double :\n",
    "            level = 49\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    def jong_yes(word):\n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 9\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 16\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 36\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 64\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "    \n",
    "    for pho in pho_list : # (2)의 결과 pho_list에 대해서\n",
    "        if len(pho) == 2 : # 받침이 없는 경우는 jong_no 함수로\n",
    "            jong_no(pho)\n",
    "        elif len(pho) == 3 : # 받침이 있는 경우는 jong_yes 함수로\n",
    "            jong_yes(pho)\n",
    "            \n",
    "    #print(level_list) 레벨 리스트 완성\n",
    "    \n",
    "    # 문장이 아예 문장부호로만 이루어진 경우 등 level_list에 아무것도 없는 경우는 제외가 필요함\n",
    "    if len(level_list)==0 :\n",
    "        print(text, \"---> 난이도 불필요 문장\")\n",
    "        pass\n",
    "    \n",
    "    else :      \n",
    "        # (4) 평균복잡도 산출\n",
    "        syl_len = len(level_list) # 음절 수 \n",
    "        #print(syl_len)\n",
    "        level_sum = sum(level_list) # 복잡도 총합\n",
    "        #print(level_sum)\n",
    "        score = level_sum / syl_len\n",
    "\n",
    "        # (5) 스케일링 및 점수화\n",
    "        score_scaled = normalizer(score) # normalizer 함수에 score 입력\n",
    "        score_per = round(score_scaled*100, 1) # 100을 곱해서 점수화\n",
    "\n",
    "        # (6) 정규화 안 하고 그냥 출력!\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 적용\n",
    "data['음운론적복잡도2'] = data['본문'].apply(text_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌3. 음절 개수, 단어 개수\n",
    "- 함수로 구현하여 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅음절 개수 count하는 함수\n",
    "def count_characters(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = cleaned_text.split()\n",
    "    total_character_count = sum(len(word) for word in words)\n",
    "    return total_character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 적용\n",
    "data['음절 개수'] = data['본문'].apply(count_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅단어 개수 count하는 함수\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터에 적용\n",
    "data['단어 개수'] = data['본문'].apply(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌4. 파생변수 생성\n",
    "- 평균 문장 길이 : 단어 개수 / 문장 개수 (='문장 당 평균 단어 개수)\n",
    "- 평균 단어 길이 : 음절 개수 / 단어 개수 (='단어 당 평균 음절 개수)\n",
    "- 꾸밈표현 등장비율 : 바른 수식언 개수 / 바른 문장 개수\n",
    "- 안은문장 등장비율 : 바른 전성어미 개수 / 바른 문장 개수\n",
    "- 이어진문장 등장비율 : 바른 연결어미 개수 / 바른 문장 개수\n",
    "- 지시표현 등장비율 : 바른 지시표현 개수 / 바른 문장 개수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅평균 문장 길이\n",
    "data['평균 문장 길이'] = data['단어 개수'] / data['바른 문장 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#✅평균 단어 길이\n",
    "data['평균 단어 길이'] = data['음절 개수'] / data['단어 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅꾸밈표현 등장비율\n",
    "data['꾸밈표현 등장비율'] = data['바른 수식언 개수'] / data['바른 문장 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅전성어미 등장비율\n",
    "data['전성어미 등장비율'] = data['바른 전성어미 개수'] / data['바른 문장 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅연결어미 등장비율\n",
    "data['연결어미 등장비율'] = data['바른 연결어미 개수'] / data['바른 문장 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅지시표현 등장비율\n",
    "data['지시표현 등장비율'] = data['바른 지시표현 개수'] / data['바른 문장 개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns # 현재 컬럼 상태 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌5. NF-iDF (News Frequency - inverse Daily Frequency)\n",
    "- 일상에서는 잘 쓰이지 않는 어휘이면서, 신문에서 많이 쓰이는 어휘는 어렵다고 판단 → 어려운 어휘가 등장하는 정도를 측정하는 지표 ‘NF-iDF’를 직접 정의함)\n",
    "- 계산 방식 : 단어마다 {**(신문기사 출현빈도/일상 사용빈도_종합)\\*해당 기사 내 출현 빈도**}를 계산 → 각 기사 본문마다 단어 난이도를 평균 내어 NF-iDF로 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------# 1. 바른으로 품사 태깅\n",
    "desired_pos_tags = ['NNG'] # 단어의 난이도 측정을 위해 명사(NNG)만 취급\n",
    "\n",
    "start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "score_column = [] # 값을 저장할 리스트\n",
    "\n",
    "#------------------------------# 2. '물결21빈도/일상빈도'를 계산\n",
    "for i, line in enumerate(data['본문'], start=1):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    res = my_tagger.tags([line])\n",
    "\n",
    "    # 'ma' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage = 0\n",
    "    \n",
    "    filtered_result = [(word, pos) for word, pos in res.pos() if pos in desired_pos_tags]\n",
    "    tokens = [word for word, _ in filtered_result] # 명사(NNG) 토큰만 꺼냄\n",
    "    \n",
    "    for token in tokens: \n",
    "        daily_freq = daily_usage.loc[daily_usage['corpus'] == token, 'frequency'].mean() #✅일상 빈도 측정\n",
    "        wave_freq = wave_corp.loc[wave_corp['단어'] == token, '총 빈도수'].mean() #✅물결21 빈도 측정\n",
    "        if np.isnan(daily_freq):\n",
    "            score = wave_freq / 1 # 분모 0 방지\n",
    "        else:\n",
    "            score = wave_freq / daily_freq\n",
    "        \n",
    "        score_tuple = (token, score) # 토큰과 score를 tuple 형태로 저장하고\n",
    "        score_list.append(score_tuple) # 리스트에 하나씩 저장\n",
    "    \n",
    "    score_column.append(score_list) # for문 다 돌아간 리스트들을 다시 최종 리스트 내에 저장\n",
    "    \n",
    "    if i % 10 == 0: # '본문'에 대한 처리 시간과 진행 상황을 모니터링\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Score Calculating Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "#✅데이터에 추가\n",
    "data['점수_tuple'] = score_column\n",
    "    \n",
    "#------------------------------# 3. 단어마다 '기사 내 출현 빈도'를 반영해보자!\n",
    "total_word_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 2] # '점수_tuple' 열에 접근\n",
    "    word_list = []\n",
    "    for (word, num) in line: # 일단 (단어,점수) 튜플에서 단어만 뽑아내기\n",
    "        word_list.append(word)\n",
    "    total_word_list.append(word_list)\n",
    "\n",
    "total_result_list = []\n",
    "\n",
    "for l in total_word_list:\n",
    "    asd = Counter(l)\n",
    "    word_frequency_pairs = list(asd.items()) # Counter 함수로 단어마다 기사 내에서 얼마나 출현했는지 빈도를 계산\n",
    "    result_list = []\n",
    "    result_list.extend(word_frequency_pairs)\n",
    "    total_result_list.append(result_list)\n",
    "    \n",
    "# 일단 counter 값을 컬럼으로 추가 (단어마다 해당 기사에 출현한 빈도를 튜플 형태로 저장)\n",
    "data['counter'] = total_result_list\n",
    "\n",
    "#------------------------------# 3-1. '물결/일상' 점수와 '기사 내 출현 빈도'를 곱하기\n",
    "result_list = []\n",
    "\n",
    "for i in range(len(data)): # 데이터셋의 모든 행에 대해\n",
    "    list1 = data.iloc[i, 2] # '점수_tuple' 열에 접근\n",
    "    list2 = data.iloc[i, 3] # 'counter' 열에 접근\n",
    "    \n",
    "    new_tuple_list = []\n",
    "\n",
    "    # 첫번째 리스트를 기준으로 루프\n",
    "    for tup1 in list1:        \n",
    "        # 튜플에서 단어 추출\n",
    "        word = tup1[0]\n",
    "\n",
    "        # 두번째 리스트에서 해당 단어에 대한 튜플 찾기\n",
    "        matching_tup2 = next((tup for tup in list2 if tup[0] == word), None)\n",
    "\n",
    "        # 만약에 찾은 경우\n",
    "        if matching_tup2 is not None:\n",
    "            # nan과 숫자를 곱해 새로운 튜플 생성\n",
    "            new_tuple = (word, matching_tup2[1] * tup1[1])\n",
    "            new_tuple_list.append(new_tuple)\n",
    "            \n",
    "    # 결과 리스트에 새 튜플리스트들을 축적!\n",
    "    result_list.append(new_tuple_list)\n",
    "    \n",
    "#✅데이터에 추가\n",
    "data['단어난이도'] = result_list\n",
    "\n",
    "#------------------------------# 4. (단어, 단어 난이도) 쌍에 대한 전처리 : nan 제거, 중복 제거, 난이도 기준 내림차순 정렬, 불용어 제거\n",
    "stopwords = ['요구', '특파원', '참석자', '희생자', '기자', '지난해', '양국', '갑', '을', '중대형', '승용차', '이듬해', '핸드볼',\n",
    "             '국가', '연합뉴스', '당국', '지난해', '기업', '상승세', '닷새', '누리집', '꼴찌', '사망자', '이날', '대통령', '지역',\n",
    "             '시인', '메시지', '센터', '시', '의료원', '붕괴', '기자실', '보고서', '소폭', '라이벌', '노조', '내년도', '견제', '앵커',\n",
    "             '논설위원', '수락', '리서치', '타임스', '무죄', '뉴시스', '도', '조사', '상당수', '지난달', '마다', '주', '가운데', '개방',\n",
    "            '연평균', '고속버스', '평균', '관계자', '고교', '연면적', '참가자', '당시'] \n",
    "\n",
    "final_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]  # '단어난이도' 열에 접근\n",
    "    line_without_nan = [(word, num) for word, num in line if not pd.isna(num)]  # tuple 내에 nan 있으면 제거\n",
    "    sorted_result = sorted(line_without_nan, key=lambda x: x[1], reverse=True)  # 최종 난이도 기준으로 내림차순 정렬\n",
    "    # 중복 제거\n",
    "    unique_values = set()\n",
    "    unique_result = [(word, num) for word, num in sorted_result if word not in unique_values and not unique_values.add(word)]\n",
    "    # 불용어 제거\n",
    "    unique_result = [(word, num) for word, num in unique_result if word not in stopwords]\n",
    "\n",
    "    final_list.append(unique_result)\n",
    "\n",
    "#✅데이터에 전처리한 값으로 갱신 (여전히 튜플 형태)\n",
    "data['단어난이도'] = final_list\n",
    "\n",
    "#------------------------------# 5. 최종 NF-iDF를 계산 (개별 단어마다 측정된 난이도들의 평균을 계산)\n",
    "NFiDF_result = []\n",
    "\n",
    "for i in range(len(data)): # 데이터셋의 모든 행에 대해\n",
    "    list1 = data.iloc[i, 4] # '단어난이도' 열에 접근\n",
    "                           \n",
    "    단어난이도_list = []\n",
    "\n",
    "    for tup1 in list1:        \n",
    "        # 튜플에서 숫자(단어별로 측정된 난이도) 추출\n",
    "        단어난이도 = tup1[1]\n",
    "        단어난이도_list.append(단어난이도)\n",
    "            \n",
    "    # 결과 리스트에 새 튜플리스트들을 축적!\n",
    "    단어난이도_sum = sum(단어난이도_list)/len(단어난이도_list)\n",
    "    NFiDF_result.append(단어난이도_sum)\n",
    "\n",
    "#✅데이터에 추가 ➡️ 최종 NF-iDF 계산 완료 !!!\n",
    "data['명사난이도 평균 점수'] = NFiDF_result\n",
    "\n",
    "\n",
    "#------------------------------#📌추가 작업\n",
    "#------------------------------# NF-iDF를 활용해서 단어장으로 제공할 단어를 추출하자!\n",
    "num_list = [] # 최종난이도만 뽑아내기\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4] # '단어난이도' 열에 접근\n",
    "    for (_, num) in line:\n",
    "        num_list.append(num)\n",
    "\n",
    "top6_list = [] # 최종난이도 상위 6개만 추출 → GPT 기반 단어 설명 예정\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]\n",
    "    lbyl_list = []\n",
    "    for (word, num) in line:\n",
    "        lbyl_list.append(word)\n",
    "        if len(lbyl_list) == 6:\n",
    "            break\n",
    "    top6_list.append(lbyl_list)\n",
    "\n",
    "#✅데이터에 추가 (=단어장 제공할 단어!!)\n",
    "data['top6'] = top6_list\n",
    "\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Final Score Calculated elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌6. 국립국어원 빈도 데이터 추가\n",
    "- 국립국어원의 '현대 국어 사용 빈도 조사(2015)' xls 파일을 발견하여 추가\n",
    "- 일상빈도를 보다 정확하게 반영하기 위해 기존 일상빈도(온라인) 변수와 국립국어원 기반 일상빈도 변수를 합쳐서 사용하기로 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('~~현대 국어 사용 빈도 조사 결과~~.csv', encoding='utf-8-sig')\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 코드(`일상빈도_온라인`)에서 필요한 부분만 재사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 값을 저장할 리스트\n",
    "average_usage_list = []\n",
    "count_list = []\n",
    "susik_list = []\n",
    "junsung_list = []\n",
    "yeongyeol_list = []\n",
    "jisi_list=[]\n",
    "s_length_list = []\n",
    "wave_list = []\n",
    "basic_list = []\n",
    "basic_count_list =[]\n",
    "\n",
    "# '본문'에 대한 처리 시간과 진행 상황을 모니터링\n",
    "for i, line in enumerate(data9181['본문'], start=1): \n",
    "    res = my_tagger.tags([line])\n",
    "    pa = res.pos() # 일상 빈도\n",
    "    ma = res.morphs() # 물결 21\n",
    "    \n",
    "#     res = my_tagger.tags([line], auto_split=True) # 문장 개수\n",
    "#     m = res.msg()\n",
    "\n",
    "    #✅일상 빈도 (국립국어원)\n",
    "    # 'pa' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_pa = 0\n",
    "\n",
    "    for word, _ in pa:\n",
    "        if word in df1['항목'].values:\n",
    "            frequency = df1.loc[df1['항목'] == word, '빈도'].sum()\n",
    "            total_usage_pa += frequency\n",
    "\n",
    "     # 중복을 포함한 형태소의 개수로 나누어 평균 일상 빈도수 계산\n",
    "    word_count = len(pa)\n",
    "    daily_average_usage = total_usage_pa / word_count\n",
    "     # 계산된 일상 빈도를 리스트에 추가\n",
    "    average_usage_list.append(daily_average_usage)\n",
    "    \n",
    "    # i가 10에 도달할 때마다 완료 메시지와 경과 시간 출력\n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터에 추가 (df_new_9520)\n",
    "data['일상빈도_국립국어원'] = average_usage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 순서 깔끔하게 정리.\n",
    "df_new_9520 = data[['언론사', '본문', '음절 개수', '단어 개수', '바른 형태소 개수', '바른 꾸밈표현 개수', '바른 지시표현 개수',\n",
    "       '바른 전성어미 개수', '바른 연결어미 개수', '바른 문장 개수', '음운론적복잡도2', '일상빈도_온라인', '일상빈도_국립국어원',\n",
    "       '물결21빈도', '기초어휘 난이도', '평균 문장 길이', '평균 단어 길이', '꾸밈표현 등장비율', '지시표현 등장비율',\n",
    "       '전성어미 등장비율', '연결어미 등장비율']]\n",
    "df_new_9520"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
