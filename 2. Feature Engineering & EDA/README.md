### 🔖목차
- [2. 변수 생성 및 EDA](#2-변수-생성-및-eda)
  - [2.1. 추가 데이터 수집](#21-추가-데이터-수집)
  - [2.2. 변수 측정 (총 17종)](#22-변수-측정-총-17종)
  - [2.3. EDA 및 추가 전처리](#23-eda-및-추가-전처리)

---

# 2. 변수 생성 및 EDA

## 2.1. 추가 데이터 수집

변수 생성을 위해 필요한 추가 데이터를 수집했습니다. 형태소 및 품사 태깅에는 전부 바른 형태소 분석기를 사용했습니다.

- ‘물결21’ 말뭉치의 형태소별 빈도수 **324,033**건 :
    - ‘신문기사 출현 빈도’를 측정하기 위한 추가 데이터
    - 2000~2013년의 4대 일간지(중앙, 동아, 조선, 한겨레) 기사를 정제한 6억 어절 말뭉치
- 국립국어원 ‘온라인 게시자료’ 말뭉치의 형태소별 빈도수 **426,636**건 :
    - ‘일상 출현 빈도’를 측정하기 위한 추가 데이터 (일상빈도_온라인 → 일상빈도_종합)
- 「현대국어 사용빈도 조사(국립국어원)」의 형태소별 빈도수 **58,433**건 :
    - ‘일상 출현 빈도’를 측정하기 위한 추가 데이터 (일상빈도_국립국어원 → 일상빈도_종합)
- 「국어 기초 어휘 선정 및 어휘 등급화 연구(국립국어원)」의 등급별 어휘 목록 **10,577**건 :
    - ‘기초 어휘 난이도’를 측정하기 위한 추가 데이터

## 2.2. 변수 측정 (총 17종)

- 수집된 데이터셋에 대해 ‘신문 독해 난이도’에 영향을 미칠 수 있는 변수들을 측정했습니다. 앞서 검토한 선행 연구들과 도메인 지식을 바탕으로 총 17종의 변수를 측정했습니다.
- 구체적인 변수 내용은 아래와 같으며, 17종 변수(a~q)를 데이터셋 **9,520행** 전체에 대해 측정하여 **이독성 변수가 포함된 데이터셋**을 확보했습니다.


### 📖 변수 종류 및 설명
- **텍스트 기초 변수** : 본문 길이에 대해 정규화되지 않은 변수들로, 요인분석에 직접 사용하지 않고 이후 파생변수 생성에 관여함

| 변수명            | 설명                                                         |
|-----------------|--------------------------------------------------------------|
| **a) 단어 개수** | 텍스트 내 공백을 기준으로 분리하여 단어 개수 추출              |
| **b) 형태소 개수** | Bareun 형태소 분석기 API를 활용해 형태소 개수 추출             |
| **c) 음절 개수** | 파이썬의 string 함수를 사용하여 음절 개수 추출                 |
| **d) 문장 개수** | Bareun 형태소 분석기 API를 활용해 문장 개수 추출               |

- **이독성 기초 변수** : 본문 길이에 대해 정규화된 변수들로, 요인분석에 직접 관여함

| 변수명            | 설명                                                         |
|-----------------|--------------------------------------------------------------|
| **e) 일상 사용빈도_온라인** | 게시판과 누리집에 등장한 형태소의 빈도 수를 총합 후, 토큰 개수로 나눈 값 (빈도 수는 국립국어원 모두의 말뭉치에서 추출) |
| **f) 일상 사용빈도_국립국어원** | 국립국어원의 <현대 국어 사용 빈도 조사 결과>에서 등장한 형태소의 빈도 수를 총합 후, 토큰 개수로 나눈 값 |
| **g) 신문기사 출현빈도** | 신문 기사에 등장한 형태소의 빈도 수를 총합 후, 토큰 개수로 나눈 값 (빈도 수는 6억 어절 규모의 ‘물결21’ 신문 코퍼스에서 추출) |
| **h) 음운론적 복잡도** | 선행 연구(윤경선 2014)의 어휘 습득 8단계를 음절마다 부여 후, 평균을 계산한 값 |
| **i) 기초어휘 난이도** | 국립국어원의 <2022 기초어휘 등급화 연구>를 바탕으로 기사의 평균 어휘 난이도를 계산한 값 (개별 난이도는 1,2,3,4등급 중 하나이며, 숫자가 클수록 어려운 단어입니다) |

- **이독성 심화 변수 :** 텍스트 난이도에 결정적이라고 판단한 파생변수를 추가로 생성함. 요인분석에 직접 관여함

| 변수명            | 설명                                                         |
|-----------------|--------------------------------------------------------------|
| **j) 평균 단어 길이** | 총 음절 개수/총 단어 개수 (=단어마다 평균적으로 몇 개의 음절이 있는가) |
| **k) 평균 문장 길이** | 총 단어 개수/총 문장 개수 (=문장마다 평균적으로 몇 개의 단어가 있는가) |
| **l) 꾸밈표현 등장비율** | 수식언 개수/총 문장 개수 (수식언 개수는 Bareun API로 카운트) |
| **m) 지시표현 등장비율** | 지시어 개수/총 문장 개수 (지시어 개수는 Bareun API로 카운트) |
| **n) 이어진문장 등장비율** | 연결어미 개수/총 문장 개수 (연결어미 개수는 Bareun API로 카운트) |
| **o) 안은문장 등장비율** | 전성어미 개수/총 문장 개수 (전성어미 개수는 Bareun API로 카운트) |
| **p) 일상 사용빈도_종합** | `일상 사용빈도_온라인`과 `일상 사용빈도_국립국어원`을 가중평균한 값 |
| **q) NF-iDF** | {신문기사 출현빈도/일상 사용빈도_종합}*해당 기사 내 출현 빈도 → 어려운 어휘가 등장하는 정도를 측정하는 지표 ‘News Frequency - inverse Daily Frequency’를 직접 정의함) |


## 2.3. EDA 및 추가 전처리

완성된 데이터셋을 분석하기 전에, 측정된 변수들에 대해 다시 EDA 및 전처리를 수행했습니다

**1) 이상치 제거**

- 전체 데이터셋에 대해 17개 변수의 분포를 시각화하여 확인한 결과, 왜도가 심한 변수들이 있어 이상치가 의심되었습니다.
- 이에 고차원 데이터에 강건한 **Isolation Forest** 방법을 사용해 이상치를 제거했습니다. (이상치 비율은 IQR 범위를 활용하여 설정했습니다)

→ 이상치가 제거된 8948행의 데이터셋을 확보했습니다.

**2) yeo-johnson 변환**

- 요인 분석(Factor Analysis)을 사용하기 위해서는 정규분포가 가정되어야 하므로, 정규분포에 가깝지 않다고 판단된 변수들은 Yeo-johnson 변환을 실시해주었습니다.
- 기술통계량, Anderson-Darling test, QQplot을 종합적으로 고려해 정규성을 판단 후, 총 5개 변수(평균 문장 길이, 꾸밈표현 등장비율, 전성어미 등장비율, 연결어미 등장비율, NF-iDF)에 대해 실시했습니다.

**3) 카테고리별 분포 차이 확인**

- 신문 카테고리별로 변수 차이가 발생할 경우, 세분화된 분석이 필요할 것이라 생각해 카테고리별 분포를 시각화하여 확인했습니다. 모든 변수에서 카테고리별 분포 차이가 크지 않았기에 별도의 데이터 분할 없이 분석을 진행했습니다.

**4) 최종 데이터셋**
- EDA 및 전처리 완료 후 최종 데이터셋은 아래와 같이 구성되었습니다.
![image](https://github.com/user-attachments/assets/8c0619e6-9fbe-4b59-a0f5-1eeb565cf837)
