{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5b8e29-b3a1-4660-bf49-1c623cc393bc",
   "metadata": {},
   "source": [
    "## ğŸ”–ëª©ì°¨\n",
    "- **1. GPT 3.5 turbo ëª¨ë¸ì— Fine-tuning ìˆ˜í–‰**\n",
    "  - Fine-tuning ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "  - Fine-tuningìš© jsonl íŒŒì¼ ìƒì„±\n",
    "  - Fine-tuning ë°ì´í„°ì…‹ì„ API í™˜ê²½ì— ì—…ë¡œë“œ\n",
    "  - Fine-tuning Job ìƒì„±\n",
    "  - Fine-tuning ì‘ì—… ìƒíƒœ í™•ì¸\n",
    "- **2. Fine-tuned ëª¨ë¸ë¡œ ë‰´ìŠ¤ë ˆí„° ìƒì„±**\n",
    "  - ì˜ˆì‹œ ë°ì´í„°ì…‹ì— ì ìš©\n",
    "  - ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„°ì…‹ì— ì ìš©\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1debfb",
   "metadata": {},
   "source": [
    "## **1. GPT 3.5 turbo ëª¨ë¸ì— Fine-tuning ìˆ˜í–‰**\n",
    "- ì¼ê´€ëœ í˜•íƒœë¡œ ì–‘ì§ˆì˜ ë‰´ìŠ¤ë ˆí„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ í•´ë‹¹ ì‘ì—…ì— íŠ¹í™”ëœ ëª¨ë¸ë¡œ ë¯¸ì„¸ ì¡°ì •í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "api_key = \"ë³¸ì¸ì˜ API keyë¥¼ ì…ë ¥\"\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81436b47",
   "metadata": {},
   "source": [
    "### 1.1. Fine-tuning ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuningì— ì‚¬ìš©í•  ë°ì´í„° 40ê±´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "data = pd.read_csv('tuning_data_ver3.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009391ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input ë°ì´í„° ì˜ˆì‹œ í™•ì¸\n",
    "print(data.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output ë°ì´í„° ì˜ˆì‹œ í™•ì¸\n",
    "print(data.iloc[0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcedc3b",
   "metadata": {},
   "source": [
    "### 1.2. Fine-tuningìš© jsonl íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77516d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠœë‹ ë°ì´í„°ë¥¼ ìœ„í•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±\n",
    "tuning_data = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "           {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë“¤ì„ ìœ„í•´ ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•´ì£¼ëŠ” ê¸°ìì…ë‹ˆë‹¤. ì–´ë ¤ìš´ ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•œë‹¤ë©´, í•™ìƒë“¤ì´ ë³¸ë¬¸ ë‚´ìš©ì„ ë”ìš± ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¡°ê±´ 1 : ëª¨ë“  ë¬¸ì¥ì—ì„œ í•™ìƒë“¤ì—ê²Œ ì¹œë°€ê° ìˆëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 2 : ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 3 : ê²°ê³¼ë¬¼ì€ 4ê°œì˜ Partìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”. êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1st Part : ê°„ë‹¨í•œ ì¸ì‚¬ì™€ í•¨ê»˜ [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ì£¼ì œì— ëŒ€í•´ì„œ ì†Œê°œí•´ì£¼ì„¸ìš”. 2nd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ ì¤‘, [ì–´ë ¤ìš´ ì–´íœ˜]ì— ì†í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 3rd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ë³¸ë¬¸ì„ ì´ˆë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¨, ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì§€ë‚˜ì¹˜ê²Œ ìš”ì•½ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë³¸ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ë³´ì¡´í•œ ìƒíƒœì—ì„œ, ì–´ë ¤ìš´ í‘œí˜„ë“¤ë§Œ ì‰½ê²Œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. 4th Part : ë³¸ë¬¸ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ì„œ ë§ˆë¬´ë¦¬ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”. ì¡°ê±´ 4 : ê¸€ì˜ ì „ë°˜ì ì¸ êµ¬ì„±ê³¼ í˜•íƒœë¥¼ ë‰´ìŠ¤ë ˆí„°ì²˜ëŸ¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": '[ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬] : {}'.format(row[\"Input\"])\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": '[ì–´ë ¤ìš´ ì–´íœ˜] : {}'.format(row[\"ì–´ë ¤ìš´ ì–´íœ˜\"])\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": row[\"ìˆ˜ì •ë³¸\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    tuning_data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2223b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json formatted data to jsonl â†’ fine_tuning_data2 â†’ fine_tuning_data3ë¡œ ì´ë¦„ ë°”ê¿ˆ\n",
    "with open(\"fine_tuning_data3.jsonl\" , encoding= \"utf-8\", mode=\"w\") as file: \n",
    "    for i in tuning_data: \n",
    "        file.write(json.dumps(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec7d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í™•ì¸(í•œ ì¤„ì”© ë¹ˆ í–‰ì´ ë‚˜ì˜¤ëŠ” ê±´ ì •ìƒ, ì› ë°ì´í„°ì—ëŠ” ë¹ˆ í–‰ ì—†ìŒ)\n",
    "with open(\"fine_tuning_data3.jsonl\") as f: \n",
    "    for line in f: print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tuning_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4544aa",
   "metadata": {},
   "source": [
    "### 1.3. Fine-tuning ë°ì´í„°ì…‹ì„ API í™˜ê²½ì— ì—…ë¡œë“œ\n",
    "- ì•ì„œ ë§Œë“  \"fine_tuning_data3.jsonl\" íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API keys\n",
    "openai.api_key = api_key\n",
    "\n",
    "# File names is 'processed_data.jsonL\n",
    "with open('fine_tuning_data3.jsonl', 'rb') as file:\n",
    "    response = openai.File.create(\n",
    "        file = file,\n",
    "        purpose = 'fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b018399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—…ë¡œë“œ ëœ íŒŒì¼ ID í™•ì¸\n",
    "file_id = response['id']\n",
    "print(f'File uploaded successfully with ID: {file_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56087e99",
   "metadata": {},
   "source": [
    "### 1.4. Fine-tuning Job ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send fine-tuning request\n",
    "tuning_job = openai.FineTuningJob.create(\n",
    "    training_file = file_id,\n",
    "    model = 'gpt-3.5-turbo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ job ID í™•ì¸\n",
    "fine_tuning_job_id = tuning_job['id']\n",
    "print(f\"Fine-tuning job created successfully with ID : {fine_tuning_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be4b16-c20d-408e-86ab-706653a515bb",
   "metadata": {},
   "source": [
    "### 1.5. Fine-tuning ì‘ì—… ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea162bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì—… ìƒíƒœ í™•ì¸\n",
    "job = openai.FineTuningJob.retrieve(id=fine_tuning_job_id)\n",
    "\n",
    "# ì‘ì—… ìƒíƒœ ì¶œë ¥ (ì‹œê°„ì€ ì¢€ ê±¸ë¦´ ìˆ˜ë„ ìˆìŒ, succeededê°€ ë‚˜ì˜¤ë©´ ì™„ë£Œ)\n",
    "print(job['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477db8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning ì™„ë£Œ í›„ ëª¨ë¸ ID í™•ì¸\n",
    "fine_tune_status = openai.FineTuningJob.retrieve(fine_tuning_job_id)\n",
    "model_id = fine_tune_status[\"fine_tuned_model\"]\n",
    "\n",
    "print(f\"Created Fine-tuned model with ID : {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83084e",
   "metadata": {},
   "source": [
    "## **2. Fine-tuning ëª¨ë¸ ì‚¬ìš©**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44ca34-8c76-4f81-a842-7fdc674ae706",
   "metadata": {},
   "source": [
    "### 2.1. ì˜ˆì‹œ ë°ì´í„°ì…‹ì— ì ìš©\n",
    "- ì‹¤ì œ ë‰´ìŠ¤ ë³¸ë¬¸ ë°ì´í„°ì…‹ì— ì ìš©í•œ ë‚´ìš©ì€ ì•„ë˜ 2.2ì ˆ ì°¸ê³ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe49ca-c5ff-4836-a7d7-14a79651aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "openai.api_key = \"â­ë³¸ì¸ì˜ API keyë¥¼ ì…ë ¥\"\n",
    "# Fine-tuned ëª¨ë¸ IDë¥¼ ì…ë ¥\n",
    "model_id = 'â­Fine-tuning ì™„ë£Œí•œ ëª¨ë¸ì˜ IDë¥¼ ì…ë ¥'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ed8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‰´ìŠ¤ë ˆí„°ë¡œ ë³€í™˜í•  ê¸°ì‚¬ ì›ë³¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„° ì‚¬ìš©)\n",
    "ex_data = pd.read_csv('ex_data.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ab300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = ex_data.iloc[0, 0] # 'ì›ë³¸ ì‹ ë¬¸ ê¸°ì‚¬'ê°€ ë‹´ê¸´ ì»¬ëŸ¼\n",
    "to_change_words = ex_data.iloc[0, 1] # 'ì„¤ëª…í•´ì•¼ í•  ì–´ë ¤ìš´ ë‹¨ì–´'ê°€ ë‹´ê¸´ ì»¬ëŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2902f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "prompt = [ \n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë“¤ì„ ìœ„í•´ ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•´ì£¼ëŠ” ê¸°ìì…ë‹ˆë‹¤. ì–´ë ¤ìš´ ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•œë‹¤ë©´, í•™ìƒë“¤ì´ ë³¸ë¬¸ ë‚´ìš©ì„ ë”ìš± ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¡°ê±´ 1 : ëª¨ë“  ë¬¸ì¥ì—ì„œ í•™ìƒë“¤ì—ê²Œ ì¹œë°€ê° ìˆëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 2 : ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 3 : ê²°ê³¼ë¬¼ì€ 4ê°œì˜ Partìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”. êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1st Part : ê°„ë‹¨í•œ ì¸ì‚¬ì™€ í•¨ê»˜ [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ì£¼ì œì— ëŒ€í•´ì„œ ì†Œê°œí•´ì£¼ì„¸ìš”. 2nd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ ì¤‘, [ì–´ë ¤ìš´ ì–´íœ˜]ì— ì†í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 3rd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ë³¸ë¬¸ì„ ì´ˆë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¨, ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì§€ë‚˜ì¹˜ê²Œ ìš”ì•½ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë³¸ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ë³´ì¡´í•œ ìƒíƒœì—ì„œ, ì–´ë ¤ìš´ í‘œí˜„ë“¤ë§Œ ì‰½ê²Œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. 4th Part : ë³¸ë¬¸ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ì„œ ë§ˆë¬´ë¦¬ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”. ì¡°ê±´ 4 : ê¸€ì˜ ì „ë°˜ì ì¸ êµ¬ì„±ê³¼ í˜•íƒœë¥¼ ë‰´ìŠ¤ë ˆí„°ì²˜ëŸ¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": '[ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬] : {}'.format(article)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": '[ì–´ë ¤ìš´ ì–´íœ˜] : {}'.format(to_change_words)\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ac32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239df8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned GPT ëª¨ë¸ì— ë©”ì‹œì§€ ë³´ë‚´ê¸°\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model_id,\n",
    "    messages=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ í™•ì¸\n",
    "chat_response = response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5566363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned GPTì˜ ë‹µë³€ ê²°ê³¼ ì¶œë ¥\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb7321-b7d1-45d5-bb05-ca8b01f37987",
   "metadata": {},
   "source": [
    "### 2.2. ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„°ì…‹ì— ì ìš©\n",
    "- ë‰´ìŠ¤ë ˆí„° ìƒì„± ì ˆì°¨\n",
    "    1. ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ ì…ë ¥ ë°›ìœ¼ë©´ ìë™ìœ¼ë¡œ ì–´ë ¤ìš´ ì–´íœ˜ë¥¼ ì‚°ì¶œí•¨\n",
    "    2. ì–´ë ¤ìš´ ì–´íœ˜ì™€ í•¨ê»˜ ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì— ì…ë ¥\n",
    "    3. ì–´ë¦°ì´ ë§ì¶¤í˜• ë‰´ìŠ¤ë ˆí„°ë¡œ ì¶œë ¥ (ì´ 2,237ê±´ ìƒì„± ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd267b0-3600-4298-acb1-ddb41cd5f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- # 0. ì–´ë ¤ìš´ ì–´íœ˜ ì‚°ì¶œì„ ìœ„í•œ calculate_noun_difficulty í•¨ìˆ˜ ì¤€ë¹„\n",
    "\n",
    "# ë°”ë¥¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì •\n",
    "import bareunpy\n",
    "from bareunpy import Tagger\n",
    "API_KEY=\"â­ë³¸ì¸ì˜ API keyë¥¼ ì…ë ¥\"\n",
    "my_tagger = Tagger(API_KEY, 'localhost')\n",
    "\n",
    "# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "\n",
    "# ë¹ˆë„ ìˆ˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "corp_freq = pd.read_csv('~~ì¼ìƒë¹ˆë„ ë°ì´í„°~~.csv', encoding = 'utf-8-sig') # ì¼ìƒ ë¹ˆë„ ë°ì´í„°\n",
    "corp_freq.drop(['Unnamed: 0', 'ratio(%)'], inplace = True, axis = 1)\n",
    "mean = np.mean(corp_freq['frequency']) # í‰ê·  ì´í•˜ í˜•íƒœì†ŒëŠ” ì˜ë¼ë‚´ê¸°\n",
    "daily_usage = corp_freq[corp_freq['frequency'] >= mean]\n",
    "\n",
    "wave_corp = pd.read_csv('~~ë¬¼ê²°21 ë¹ˆë„ ë°ì´í„°~~.csv', encoding='utf-8-sig') # ë¬¼ê²° 21 ë¹ˆë„ ë°ì´í„°\n",
    "wave_corp.drop(['Unnamed: 0.1', 'Unnamed: 0'], inplace = True, axis = 1)\n",
    "\n",
    "# ë¶ˆìš©ì–´ ì •ì˜\n",
    "stopwords = ['ìš”êµ¬', 'íŠ¹íŒŒì›', 'ì°¸ì„ì', 'í¬ìƒì', 'ê¸°ì', 'ì§€ë‚œí•´', 'ì–‘êµ­', 'ê°‘', 'ì„', 'ì¤‘ëŒ€í˜•', 'ìŠ¹ìš©ì°¨', 'ì´ë“¬í•´', 'í•¸ë“œë³¼',\n",
    "             'êµ­ê°€', 'ì—°í•©ë‰´ìŠ¤', 'ë‹¹êµ­', 'ì§€ë‚œí•´', 'ê¸°ì—…', 'ìƒìŠ¹ì„¸', 'ë‹·ìƒˆ', 'ëˆ„ë¦¬ì§‘', 'ê¼´ì°Œ', 'ì‚¬ë§ì', 'ì´ë‚ ', 'ëŒ€í†µë ¹', 'ì§€ì—­',\n",
    "             'ì‹œì¸', 'ë©”ì‹œì§€', 'ì„¼í„°', 'ì‹œ', 'ì˜ë£Œì›', 'ë¶•ê´´', 'ê¸°ìì‹¤', 'ë³´ê³ ì„œ', 'ì†Œí­', 'ë¼ì´ë²Œ', 'ë…¸ì¡°', 'ë‚´ë…„ë„', 'ê²¬ì œ', 'ì•µì»¤',\n",
    "             'ë…¼ì„¤ìœ„ì›', 'ìˆ˜ë½', 'ë¦¬ì„œì¹˜', 'íƒ€ì„ìŠ¤', 'ë¬´ì£„', 'ë‰´ì‹œìŠ¤', 'ë„', 'ì¡°ì‚¬', 'ìƒë‹¹ìˆ˜', 'ì§€ë‚œë‹¬', 'ë§ˆë‹¤', 'ì£¼', 'ê°€ìš´ë°', 'ê°œë°©',\n",
    "            'ì—°í‰ê· ', 'ê³ ì†ë²„ìŠ¤', 'í‰ê· ', 'ê´€ê³„ì', 'ê³ êµ', 'ì—°ë©´ì ', 'ì°¸ê°€ì', 'ë‹¹ì‹œ', 'ì£¼ì„', 'ì„ ìˆ˜ë‹¨']\n",
    "\n",
    "# ê° ê¸°ì‚¬ ë³¸ë¬¸ì˜ NF-iDFë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def calculate_noun_difficulty(input_data, daily_usage, wave_corp, my_tagger, desired_pos_tags=['NNG'], stopwords=None):\n",
    "    start_time = time.time()\n",
    "    score_column = []\n",
    "\n",
    "    for i, line in enumerate(input_data['ë³¸ë¬¸'], start=1):\n",
    "        score_list = []\n",
    "        res = my_tagger.tags([line])\n",
    "\n",
    "        total_usage = 0\n",
    "        filtered_result = [(word, pos) for word, pos in res.pos() if pos in desired_pos_tags]\n",
    "        tokens = [word for word, _ in filtered_result]\n",
    "\n",
    "        for token in tokens:\n",
    "            daily_freq = daily_usage.loc[daily_usage['corpus'] == token, 'frequency'].mean()\n",
    "            wave_freq = wave_corp.loc[wave_corp['ë‹¨ì–´'] == token, 'ì´ ë¹ˆë„ìˆ˜'].mean()\n",
    "\n",
    "            if np.isnan(daily_freq):\n",
    "                score = wave_freq / 1\n",
    "            else:\n",
    "                score = wave_freq / daily_freq\n",
    "\n",
    "            score_tuple = (token, score)\n",
    "            score_list.append(score_tuple)\n",
    "\n",
    "        score_column.append(score_list)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Score Calculating Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    input_data['ì ìˆ˜_tuple'] = score_column\n",
    "\n",
    "    total_word_list = []\n",
    "    for i in range(len(input_data)):\n",
    "        line = input_data.loc[i, 'ì ìˆ˜_tuple']\n",
    "        word_list = [word for (word, _) in line]\n",
    "        total_word_list.append(word_list)\n",
    "\n",
    "    input_data['counter'] = total_word_list\n",
    "\n",
    "    total_result_list = []\n",
    "    for l in input_data['counter']:\n",
    "        asd = Counter(l)\n",
    "        word_frequency_pairs = list(asd.items())\n",
    "        result_list = []\n",
    "        result_list.extend(word_frequency_pairs)\n",
    "        total_result_list.append(result_list)\n",
    "\n",
    "    input_data['counter'] = total_result_list\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        list1 = input_data.loc[i, 'ì ìˆ˜_tuple']\n",
    "        list2 = input_data.loc[i, 'counter']\n",
    "\n",
    "        new_tuple_list = []\n",
    "\n",
    "        for tup1 in list1:\n",
    "            word = tup1[0]\n",
    "            matching_tup2 = next((tup for tup in list2 if tup[0] == word), None)\n",
    "\n",
    "            if matching_tup2 is not None:\n",
    "                new_tuple = (word, matching_tup2[1] * tup1[1])\n",
    "                new_tuple_list.append(new_tuple)\n",
    "\n",
    "        result_list.append(new_tuple_list)\n",
    "\n",
    "    input_data['ë‹¨ì–´ë‚œì´ë„'] = result_list\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        line = input_data.loc[i, 'ë‹¨ì–´ë‚œì´ë„']\n",
    "        line_without_nan = [(word, num) for word, num in line if not pd.isna(num)]\n",
    "        sorted_result = sorted(line_without_nan, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        unique_values = set()\n",
    "        unique_result = [(word, num) for word, num in sorted_result if word not in unique_values and not unique_values.add(word)]\n",
    "\n",
    "        if stopwords:\n",
    "            unique_result = [(word, num) for word, num in unique_result if word not in stopwords]\n",
    "\n",
    "        final_list.append(unique_result)\n",
    "\n",
    "    input_data['ë‹¨ì–´ë‚œì´ë„'] = final_list\n",
    "\n",
    "    num_list = []\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        line = input_data.loc[i, 'ë‹¨ì–´ë‚œì´ë„']\n",
    "        for (_, num) in line:\n",
    "            num_list.append(num)\n",
    "\n",
    "    top6_list = []\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        line = input_data.loc[i, 'ë‹¨ì–´ë‚œì´ë„']\n",
    "        lbyl_list = []\n",
    "        for (word, num) in line:\n",
    "            lbyl_list.append(word)\n",
    "            if len(lbyl_list) == 6:\n",
    "                break\n",
    "        top6_list.append(lbyl_list)\n",
    "\n",
    "    input_data['top6'] = top6_list #ğŸ‘ˆGPTì— ì „ë‹¬í•  ì–´ë ¤ìš´ ì–´íœ˜ Top6\n",
    "\n",
    "    final_result = []\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        list1 = input_data.loc[i, 'ë‹¨ì–´ë‚œì´ë„']\n",
    "        ë‹¨ì–´ë‚œì´ë„_list = [tup[1] for tup in list1]\n",
    "\n",
    "        ë‹¨ì–´ë‚œì´ë„_sum = sum(ë‹¨ì–´ë‚œì´ë„_list) / len(ë‹¨ì–´ë‚œì´ë„_list)\n",
    "        final_result.append(ë‹¨ì–´ë‚œì´ë„_sum)\n",
    "\n",
    "    input_data['ëª…ì‚¬ë‚œì´ë„ í‰ê·  ì ìˆ˜'] = final_result #ğŸ‘ˆê¸°ì‚¬ë³„ NF-iDF ì¸¡ì •\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Final Score Calculated elapsed time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129c963-103f-43cd-ae56-d6cec0533d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- # 1. ì‹¤ì œ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì–´ë ¤ìš´ ì–´íœ˜ ì‚°ì¶œ\n",
    "\n",
    "# ë‰´ìŠ¤ë ˆí„°ë¡œ ë³€í™˜í•  ì‹ ë¬¸ ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ\n",
    "input_data = pd.read_csv('~~ì‹ ë¬¸ê¸°ì‚¬ ì›ë³¸ ë°ì´í„°ì…‹~~.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "# calculate_noun_difficulty í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ 'ì–´ë ¤ìš´ ì–´íœ˜ Top6'ë¥¼ ì‚°ì¶œ\n",
    "result_data = calculate_noun_difficulty(input_data=input_data, \n",
    "                                        daily_usage=daily_usage, \n",
    "                                        wave_corp=wave_corp, \n",
    "                                        my_tagger=my_tagger, \n",
    "                                        stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581e27e-6d39-4499-873b-716ab4a5d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- # 2. ì‹ ë¬¸ê¸°ì‚¬ & ì–´ë ¤ìš´ ì–´íœ˜ë¥¼ Fine-tuned GPTì— ì „ë‹¬\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ gpt_answer ì—´ ì¶”ê°€\n",
    "result_data['gpt_answer'] = ''\n",
    "\n",
    "start_time = time.time()\n",
    "for index, row in result_data.iterrows(): \n",
    "    article = row['ë³¸ë¬¸'] #ğŸ‘ˆFine-tuned GPTì—ê²Œ ì…ë ¥í•  'ê¸°ì‚¬ ë³¸ë¬¸'\n",
    "    to_change_words = row['top6'] #ğŸ‘ˆFine-tuned GPTì—ê²Œ ì…ë ¥í•  'ì–´ë ¤ìš´ ì–´íœ˜'\n",
    "    \n",
    "    to_change_again = []\n",
    "    for word in to_change_words :\n",
    "        to_change_again.append(word)\n",
    "    \n",
    "    to_change_again = ', '.join(to_change_again)\n",
    "\n",
    "    # prompt ì •ì˜\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë“¤ì„ ìœ„í•´ ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•´ì£¼ëŠ” ê¸°ìì…ë‹ˆë‹¤. ì–´ë ¤ìš´ ì‹ ë¬¸ ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‘ì„±í•œë‹¤ë©´, í•™ìƒë“¤ì´ ë³¸ë¬¸ ë‚´ìš©ì„ ë”ìš± ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ë¥¼ ë‰´ìŠ¤ë ˆí„° í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¡°ê±´ 1 : ëª¨ë“  ë¬¸ì¥ì—ì„œ í•™ìƒë“¤ì—ê²Œ ì¹œë°€ê° ìˆëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 2 : ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ì¡°ê±´ 3 : ê²°ê³¼ë¬¼ì€ 4ê°œì˜ Partìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”. êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1st Part : ê°„ë‹¨í•œ ì¸ì‚¬ì™€ í•¨ê»˜ [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ì£¼ì œì— ëŒ€í•´ì„œ ì†Œê°œí•´ì£¼ì„¸ìš”. 2nd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ ì¤‘, [ì–´ë ¤ìš´ ì–´íœ˜]ì— ì†í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 3rd Part : [ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬]ì˜ ë³¸ë¬¸ì„ ì´ˆë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¨, ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì§€ë‚˜ì¹˜ê²Œ ìš”ì•½ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë³¸ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ë³´ì¡´í•œ ìƒíƒœì—ì„œ, ì–´ë ¤ìš´ í‘œí˜„ë“¤ë§Œ ì‰½ê²Œ í’€ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. 4th Part : ë³¸ë¬¸ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ì„œ ë§ˆë¬´ë¦¬ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”. ì¡°ê±´ 4 : ê¸€ì˜ ì „ë°˜ì ì¸ êµ¬ì„±ê³¼ í˜•íƒœë¥¼ ë‰´ìŠ¤ë ˆí„°ì²˜ëŸ¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '[ì›ë³¸ ì‹ ë¬¸ê¸°ì‚¬] : {}'.format(article)\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '[ì–´ë ¤ìš´ ì–´íœ˜] : {}'.format(to_change_again)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Fine-tuned GPTì— ì¿¼ë¦¬ ë³´ë‚´ê¸°\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=prompt,\n",
    "        model=model_id,\n",
    "    )\n",
    "\n",
    "    # Fine-tuned GPTì˜ ë‹µë³€ì„ ë°›ì•„ì„œ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "    result_data.at[index, 'gpt_answer'] = chat_completion.choices[0].message.content\n",
    "    \n",
    "    # ìƒ˜í”Œ 10ê°œë‹¹ ì‹œê°„ ì¸¡ì •\n",
    "    if index % 1 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {index}th samples. Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f1098-dfc3-44b7-a145-05bc82f59478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- # 3. ì–´ë¦°ì´ ë§ì¶¤í˜• ë‰´ìŠ¤ë ˆí„° 2,237ê±´ ì¶œë ¥ ì™„ë£Œ \n",
    "\n",
    "result_data.head() # ê²°ê³¼ í™•ì¸\n",
    "result_data.to_csv('~~ë‰´ìŠ¤ë ˆí„° ìƒì„± ì™„ë£Œ ë°ì´í„°ì…‹~~.csv', encoding = 'utf-8-sig', index = False) # CSV íŒŒì¼ë¡œ ì €ì¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
